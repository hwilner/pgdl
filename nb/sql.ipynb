{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc6171-a93c-4fc5-bb8d-7f63daa2aab5",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import json, requests, time\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "engine = create_engine('postgresql://postgres:argmax@pg:5432/postgres')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c6725",
   "metadata": {},
   "source": [
    "# Data\n",
    "Every time a user opens a mobile app, an auction is going on behind the scenes. The highest bidder gets to advertise his ad to the user.\n",
    "## Auctions Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c86e9",
   "metadata": {},
   "source": [
    "## App Vectors table\n",
    "We've gathered the first few sentences from the app store description and embedded it with a [model](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05408c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = f'''\n",
    "SELECT\n",
    "    *\n",
    "FROM app_vectors\n",
    "'''\n",
    "has_embedding = False\n",
    "while not has_embedding:\n",
    "    with engine.connect() as db_con:\n",
    "        df = pd.read_sql(sql_query, con=db_con)\n",
    "    has_embedding = (~df[\"embedding\"].isna()).all()\n",
    "    if not has_embedding:\n",
    "        print(\"Waiting for embeddings...\")\n",
    "        time.sleep(15)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac0d2f",
   "metadata": {},
   "source": [
    "We can use the `<=>` operator to run vector search within the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79504473",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vec = json.loads(df.embedding[0]) # get the first embedding\n",
    "print (\"Embedding size: {l}\".format(l=len(vec)))\n",
    "\n",
    "sql_query = f'''\n",
    "SELECT\n",
    "    \"bundleId\"\n",
    "FROM app_vectors\n",
    "ORDER BY embedding<=>'{json.dumps(vec)}'\n",
    "'''\n",
    "with engine.connect() as db_con:\n",
    "    df = pd.read_sql(sql_query, con=db_con)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7be478",
   "metadata": {},
   "source": [
    "# What you need to do\n",
    "## The hypothesis\n",
    "We assume that apps with similar desciptions, would have a similar asking price in the auctions (`sentPrice` column).\n",
    "\n",
    "Use cosine similarity (`<=>`) on the embeddings to find similar apps, and any statistical tools you find suitable to prove or disprove this hypothesis.\n",
    "\n",
    "## Is it consistent?\n",
    "There are several other features in the auctions table (such as `CountryCode` and `OS`), \n",
    "Do your findings hold for those as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744eaece-d4d5-4ba1-af10-faca4b20f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # No need to convert to np.array as the inputs are already numpy arrays\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4bace-90fd-40d7-aff5-3231351b208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine('postgresql://postgres:argmax@pg:5432/postgres')\n",
    "\n",
    "# Query to load the auctions table\n",
    "auctions_query = 'SELECT * FROM auctions;'\n",
    "auctions_df = pd.read_sql(auctions_query, con=engine)\n",
    "\n",
    "# Query to load the app_vectors table\n",
    "vectors_query = 'SELECT * FROM app_vectors;'\n",
    "vectors_df = pd.read_sql(vectors_query, con=engine)\n",
    "\n",
    "# Determine if embeddings are ready in the vectors_df\n",
    "has_embedding = (~vectors_df[\"embedding\"].isna()).all()\n",
    "if not has_embedding:\n",
    "    print(\"Waiting for embeddings...\")\n",
    "    # You might need to loop and check periodically if embeddings are ready\n",
    "    # or handle this logic according to your application's requirements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c7e6b-b841-4816-9c38-65e84a45a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embeddings\n",
    "embeddings = [np.array(json.loads(e)) for e in vectors_df['embedding']]\n",
    "\n",
    "# Initialize similarity matrix\n",
    "similarity_matrix = np.zeros((len(embeddings), len(embeddings)))\n",
    "\n",
    "# Calculate the similarity matrix\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i + 1, len(embeddings)):\n",
    "        similarity = cosine_similarity(embeddings[i], embeddings[j])\n",
    "        similarity_matrix[i, j] = similarity_matrix[j, i] = similarity\n",
    "\n",
    "# Fill the diagonal of the similarity matrix with 1s\n",
    "np.fill_diagonal(similarity_matrix, 1)\n",
    "\n",
    "# Now similarity_matrix is the cosine similarity matrix for the vectors\n",
    "\n",
    "# Convert the similarity matrix to a DataFrame for easier processing\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=vectors_df['bundleId'], columns=vectors_df['bundleId'])\n",
    "# Calculate the average similarity \n",
    "average_similarity = similarity_matrix.mean(axis=1)\n",
    "\n",
    "vectors_df['average_similarity'] = average_similarity\n",
    "\n",
    "merged_df = pd.merge(auctions_df, vectors_df[['bundleId', 'average_similarity']], on='bundleId', how='inner')\n",
    "# Calculate the correlation\n",
    "correlation = merged_df[['average_similarity', 'sentPrice']].corr().iloc[0, 1]\n",
    "print(\"Correlation between average description similarity and sentPrice:\", correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3f295-c28b-4ee5-9f26-a3173710dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_similarity\n",
    "similarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d004b2-0010-4aeb-bbae-291a652f3763",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9867c8-0583-4a91-9702-45c485befca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between the average similarity and sentPrice in the merged dataframe\n",
    "correlation_with_price = merged_df[['average_similarity', 'sentPrice']].corr().iloc[0, 1]\n",
    "print(\"Correlation with sentPrice:\", correlation_with_price)\n",
    "\n",
    "# To check for consistency with other features, calculate the correlation for each feature\n",
    "for feature in ['countryCode', 'osAndVersion']:\n",
    "    merged_df[f'average_similarity_{feature}'] = merged_df.groupby(feature)['average_similarity'].transform('mean')\n",
    "    correlation_with_feature = merged_df.groupby(feature).apply(\n",
    "        lambda x: x[['average_similarity', 'sentPrice']].corr().iloc[0, 1]\n",
    "    )\n",
    "    print(f\"Correlation with {feature}:\", correlation_with_feature.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2c2ac-9799-4476-8a66-6d12c614382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise_correlations = []\n",
    "# for i in range(len(merged_df)):\n",
    "#     for j in range(i+1, len(merged_df)):\n",
    "#         price_difference = abs(merged_df.loc[i, 'sentPrice'] - merged_df.loc[j, 'sentPrice'])\n",
    "#         description_similarity = merged_df.loc[i, 'average_similarity']\n",
    "#         pairwise_correlations.append((description_similarity, price_difference))\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# pairwise_df = pd.DataFrame(pairwise_correlations, columns=['description_similarity', 'price_difference'])\n",
    "\n",
    "# # Calculate correlation\n",
    "# pairwise_correlation = pairwise_df.corr().iloc[0, 1]\n",
    "# print(\"Pairwise correlation between description similarity and price difference:\", pairwise_correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef44438-1fae-4d34-8966-7c4173675e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so all rows are too long to compute..so lets try Binning Similarity Scores\n",
    "# by groupi the similarity scores into bins (e.g., high, medium, low similarity) and analyze the variation in sentPrice within these bins instead of using a continuous similarity measure.\n",
    "\n",
    "# for simplicity:\n",
    "df = merged_df\n",
    "# Bin the average similarity scores into quantiles\n",
    "try:\n",
    "    df['similarity_bin'] = pd.qcut(df['average_similarity'], q=10, labels=False, duplicates='drop')\n",
    "except ValueError as e:\n",
    "    print(\"ValueError:\", e)\n",
    "    # If an error occurs (e.g., due to too few unique quantiles), handle it accordingly\n",
    "    # For example, you might decide to use fewer quantiles\n",
    "    df['similarity_bin'] = pd.qcut(df['average_similarity'], q=5, labels=False, duplicates='drop')\n",
    "\n",
    "# Group by the similarity bin and calculate the mean sentPrice for each bin\n",
    "mean_sent_price_by_bin = df.groupby('similarity_bin')['sentPrice'].mean()\n",
    "\n",
    "# Now, see if there's a trend in the mean sentPrice across the bins\n",
    "trend = mean_sent_price_by_bin.sort_index()\n",
    "\n",
    "# Optionally, perform a polynomial fit to see the trend more clearly\n",
    "coefficients = np.polyfit(trend.index, trend.values, deg=1)\n",
    "poly_fit = np.poly1d(coefficients)\n",
    "\n",
    "# Print the coefficients or use them to plot the trend line\n",
    "print(f\"Trend line coefficients: {coefficients}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf08a70-957f-4ef5-9a27-a22ca48916dc",
   "metadata": {},
   "source": [
    "We got a slope coefficient of 0.09433349 and an intercept of 0.92230247 from our trend line analysis. Let's break down what this means for our dataset:\n",
    "\n",
    "Slope (0.09433349): This number tells us about the relationship between the similarity scores and sentPrice. A positive slope like the one we got, approximately 0.094, indicates a slight upward trend. This suggests that apps with higher similarity scores might be associated with a modestly higher sentPrice on average.\n",
    "\n",
    "Intercept (0.92230247): This value predicts the sentPrice for the baseline case where the similarity score bin is zero. So, if an app's description had the lowest level of similarity compared to others (assuming the lowest bin is coded as zero), our model would predict its sentPrice to be around 0.922.\n",
    "\n",
    "The equation we've derived for the trend line is sentPrice = 0.09433349 * similarity_bin + 0.92230247, giving us a linear model to estimate sentPrice from our binned similarity scores.\n",
    "\n",
    "We should approach these results with a degree of caution. The coefficients provide an estimate but don't account for the variability of the data or the significance of the trend. To better understand the model's accuracy, we'd need to look at the R-squared value or conduct hypothesis testing on the coefficients to see if they're statistically significant and not due to random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cae131-bc2c-460c-80d2-9f88dfcdde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_regression(X, y, num_iterations=5, k=4.685):\n",
    "    weights = np.ones(len(y))\n",
    "    for iteration in range(num_iterations):\n",
    "        # Apply weights to X and y\n",
    "        XW = X * np.sqrt(weights[:, np.newaxis])\n",
    "        yW = y * np.sqrt(weights)\n",
    "        \n",
    "        # Solve the weighted least squares problem\n",
    "        beta = np.linalg.lstsq(XW, yW, rcond=None)[0]\n",
    "        \n",
    "        # Calculate residuals and update weights\n",
    "        residuals = y - X.dot(beta)\n",
    "        mad = np.median(np.abs(residuals - np.median(residuals)))\n",
    "        adjusted_residuals = np.sqrt(np.abs(residuals)) / (mad + np.finfo(float).eps)\n",
    "        weights = (1 - (adjusted_residuals / k)**2)**2\n",
    "        weights[adjusted_residuals >= k] = 0\n",
    "    return beta\n",
    "\n",
    "results = []  # Initialize the results list\n",
    "\n",
    "x = df['average_similarity'].values\n",
    "y = df['sentPrice'].values\n",
    "\n",
    "for degree in range(2, 11):\n",
    "    # Generate polynomial features\n",
    "    X_poly = np.vander(x, degree + 1, increasing=True)\n",
    "    \n",
    "    # Apply robust regression to the polynomial features\n",
    "    beta_poly = robust_regression(X_poly, y)\n",
    "    \n",
    "    # Predict y using the model\n",
    "    y_pred = X_poly.dot(beta_poly)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    # Calculate R-squared\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((y - np.mean(y))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    results.append((degree, r_squared))\n",
    "    \n",
    "    # Print the results for each polynomial degree\n",
    "    print(f\"Degree: {degree}, R-squared: {r_squared}\")\n",
    "    print(f\"Degree: {degree}, Coefficients: {beta_poly}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cae14-5ce5-4c32-a1c0-761369757091",
   "metadata": {},
   "source": [
    "The R-squared values we got are negative across all polynomial degrees tested, from 2 through 10. Typically, an R-squared value aims to measure the proportion of variance in the dependent variable that is predictable from the independent variable(s). Negative R-squared values suggest that our polynomial models fit the data worse than a simple horizontal line at the mean of sentPrice. This implies that none of the models provide a satisfactory explanation for the variance in our target variable.\r\n",
    "\r\n",
    "As for the significance of the model coefficients (the beta values for each polynomial degree), determining their significance usually involves examining p-values from hypothesis tests, such as the t-test, for each coefficient to assess if they significantly differ from zero. Without conducting these tests and calculating p-values, we can't directly assess the significance of the coefficients from the provided results.\r\n",
    "\r\n",
    "The outcomes of our robust regression and polynomial modeling efforts indicate that these approaches have not yielded a model that effectively captures the relationship between our predictor(s) and the target variabhis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d467d04-30ae-46c6-a6ad-143fb3617978",
   "metadata": {},
   "source": [
    "Given our dataset with over 41,000 rows, we opted not to implement the k-nearest neighbors (KNN) regression, a form of non-parametric regression. While KNN regression can be quite intuitive and straightforward, involving averaging the target values of the k nearest points in the feature space for each prediction point, it faces significant scalability challenges, especially with large datasets like ours.\n",
    "\n",
    "The primary reason for this decision is the computational complexity associated with KNN. For each prediction, KNN requires calculating the distance from the prediction point to all other points in the dataset, identifying the k nearest neighbors based on these distances, and then averaging their target values. This process becomes increasingly time-consuming as the dataset grows, making it impractical for datasets with tens of thousands of observations or more.\n",
    "\n",
    "Additionally, while KNN is a flexible modeling approach that can capture complex relationships without assuming a specific functional form, it lacks the statistical rigor of more advanced non-parametric methods, such as kernel regression or LOESS (Locally Estimated Scatterplot Smoothing). These methods involve more sophisticated techniques for estimating the regression function, which can provide a better fit to the data but also require significant computational resources.\n",
    "\n",
    "Here's a brief overview of how KNN regression could be implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b6f9a-5a8a-403e-9c96-26bc02a1959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_regression(predict_point, features, targets, k):\n",
    "    # Calculate distances from predict_point to all other points\n",
    "    distances = np.linalg.norm(features - predict_point, axis=1)\n",
    "    \n",
    "    # Find the indices of the k nearest neighbors\n",
    "    k_indices = np.argpartition(distances, k)[:k]\n",
    "    \n",
    "    # Average the target values of the nearest neighbors\n",
    "    return np.mean(targets[k_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e04d35-6a66-4ca9-be66-22c9bb2a1b8a",
   "metadata": {},
   "source": [
    "While the KNN approach remains a valuable tool for certain applications, especially when dealing with smaller datasets or when computational resources are not a constraint, it's not the most efficient choice for our current scenario. The computational demands and time complexity issues associated with applying KNN to our large dataset led us to explore alternative modeling strategies that could offer a more practical balance between model performance and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09038a2d-9362-4c93-b918-63f18d844a68",
   "metadata": {},
   "source": [
    "we can try exploring more complex relationships, such as how the bidFloorPrice might interact with the similarity scores and affect the sentPrice, is a valuable next step. This approach allows us to investigate whether there's an interaction effect that could explain the variance in sentPrice more effectively. By considering bidFloorPrice in our model, we're essentially looking to see if the relationship between app description similarity and sentPrice changes at different levels of bidFloorPrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9f9e6-52dd-46c9-b99c-a80001aa2708",
   "metadata": {},
   "outputs": [],
   "source": [
    "bidFloorPrice = df['bidFloorPrice'].values\n",
    "\n",
    "# Let's consider a simple model with a quadratic term for similarity and its interaction with bidFloorPrice\n",
    "X = np.vstack([\n",
    "    np.ones(len(x)),  # Intercept\n",
    "    x,  # Linear term for similarity\n",
    "    x**2,  # Quadratic term for similarity\n",
    "    bidFloorPrice,  # Linear term for bidFloorPrice\n",
    "    x * bidFloorPrice,  # Interaction term\n",
    "]).T\n",
    "\n",
    "# Fit the model\n",
    "beta = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "# Predictions\n",
    "y_pred = X.dot(beta)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y - y_pred\n",
    "\n",
    "# Compute R-squared\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y - np.mean(y))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"Model coefficients: {beta}\")\n",
    "print(f\"R-squared: {r_squared}\") \t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ccf6ce-6896-4fdb-b342-26476cfe95b4",
   "metadata": {},
   "source": [
    "Model Coefficients: The model coefficients for our regression model are [30.43419184, -125.00554341, 130.34515304, 490.4529712, -874.71697674]. These coefficients correspond to the intercept, the linear term for similarity scores, the quadratic term for similarity scores, the linear term for bidFloorPrice, and the interaction term between similarity scores and bidFloorPrice, respectively.\r\n",
    "\r\n",
    "R-squared Value: The R-squared value is 0.0152058833363653. This value indicates that the model explains approximately 1.52% of the variance in the sentPrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b2dab-7f39-4832-8cba-35b067b6a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'eventTimestamp' to numeric (float or int), then convert from milliseconds to seconds\n",
    "df['eventTimestamp'] = pd.to_numeric(df['eventTimestamp']) / 1000\n",
    "\n",
    "# Now convert 'eventTimestamp' to datetime format\n",
    "df['eventTimestamp'] = pd.to_datetime(df['eventTimestamp'], unit='s')\n",
    "\n",
    "# Proceed with extracting time-based features\n",
    "df['hour'] = df['eventTimestamp'].dt.hour\n",
    "df['day_of_week'] = df['eventTimestamp'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Add an intercept and other predictors to X as needed\n",
    "X = df[['hour', 'is_weekend']]  # You can add more features based on your analysis needs\n",
    "y = df['sentPrice']\n",
    "\n",
    "# Add an intercept\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Fit the model\n",
    "beta = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "# Model evaluation: Calculate predictions and R-squared\n",
    "y_pred = X.dot(beta)\n",
    "residuals = y - y_pred\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y - np.mean(y))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"Model coefficients: {beta}\")\n",
    "print(f\"R-squared: {r_squared}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0c6c0-15f4-4a4d-b361-cf97500968bf",
   "metadata": {},
   "source": [
    "The R-squared value of 0.00026574580231197853 is very close to zero, indicating that our model explains only a tiny fraction of the variance in sentPrice. This suggests that the features we've included (hour and is_weekend) do not strongly predict sentPrice on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1b631-0311-43e8-bea8-90596d15f3c2",
   "metadata": {},
   "source": [
    "lets try to analyze the impact of the 'osAndVersion' column, which is a string indicating the operating system and its version, could provide insights into how different OS versions affect sentPrice. Since osAndVersion is categorical, we would first need to convert it into a format suitable for regression analysis. This typically involves creating dummy variables (also known as one-hot encoding) for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40cd383-d9f3-44ed-933c-33e9a07deb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'osAndVersion' into dummy variables, automatically dropping the first category\n",
    "os_version_dummies = pd.get_dummies(df['osAndVersion'], prefix='os_version', drop_first=True)\n",
    "\n",
    "# Assuming 'sentPrice' is your target variable\n",
    "y = df['sentPrice'].values\n",
    "\n",
    "# If you have other features you want to include, prepare them\n",
    "# Here, we'll proceed with only the 'os_version' dummies for demonstration purposes\n",
    "# Make sure to replace 'X_other' with actual other features if available\n",
    "X_other = pd.DataFrame()  # This is a placeholder; replace with your actual other features\n",
    "\n",
    "# Directly use the dummies as part of your features matrix if not joining back to 'df'\n",
    "X = os_version_dummies.values\n",
    "\n",
    "# Assuming X_other is not just an empty DataFrame, you would concatenate it like so:\n",
    "# X = np.hstack([X_other.values, X])\n",
    "\n",
    "# Fit the model\n",
    "# Adding an intercept to X\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "beta = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "# Model evaluation: Calculate predictions and R-squared\n",
    "y_pred = X.dot(beta)\n",
    "residuals = y - y_pred\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y - np.mean(y))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"Model coefficients: {beta}\")\n",
    "print(f\"R-squared: {r_squared}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7acfdf5-115d-466b-bb45-1f8eb305ee40",
   "metadata": {},
   "source": [
    "An R-squared value of 0.010466412999634023 means that approximately 1.05% of the variability in sentPrice is explained by the OS and version categories. While we have successfully included the osAndVersion variable in our model, the low R-squared value indicates that this model, as it stands, explains only a small fraction of the variance in sentPrice.\r\n",
    "\r\n",
    "Considerations:\r\n",
    "Complexity vs. Insight: The incorporation of many dummy variables for osAndVersion adds complexity to the model. Although we now have insights into how different OS versions might affect sentPrice, the overall explanatory power of the model remains low.\r\n",
    "Potential for Overfitting: With a large number of coefficients relative to the amount of data, there's a risk of overfitting, especially when the R-squared is low. It's essential to validate the model on a separate test set or use cross-validation techniques to assess its predictive performance.\r\n",
    "Further Exploration Needed: The low R-squared value suggests that additional factors not captured by the osAndVersion alone significantly influence sentPrice. It might be beneficial to explore other variables, interaction terms, or even non-linear models to improve the model's explanatory power.\r\n",
    "Domain Knowledge and Feature Engineering: Further domain knowledge could inform more nuanced feature engineering, such as grouping similar OS versions or extracting broader categories (e.g., iOS vs. Android) that might have more substantial effects on sentPrice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec8da96-462d-4b22-9d42-813c15d40ad6",
   "metadata": {},
   "source": [
    "Significant coefficients, especially those with large magnitudes, do indicate that specific OS versions have a stronger relationship with sentPrice than others. Analyzing these coefficients can provide insights into which OS versions are associated with higher or lower prices, potentially pointing to preferences or trends in the dataset that could inform strategic decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccef72f-4fdf-4897-81d1-3fdf6d542d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1936ae2e-915e-40a3-a952-ad9e148c8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets calculate the R-squared value manually:\n",
    "x = df['similarity_bin'].values\n",
    "y = df['sentPrice'].values\n",
    "y_pred = coefficients[0] * x + coefficients[1]  # The predicted sentPrice values\n",
    "\n",
    "# Calculate R-squared manually\n",
    "ss_res = np.sum((y - y_pred) ** 2)  # Sum of squares of residuals\n",
    "ss_tot = np.sum((y - np.mean(y)) ** 2)  # Total sum of squares\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"R-squared value: {r_squared}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77572aa6-1289-43be-9ec2-6a337da4e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values\n",
    "y_pred = coefficients[0] * x + coefficients[1]\n",
    "\n",
    "# Residuals (errors) between actual and predicted values\n",
    "residuals = y - y_pred\n",
    "\n",
    "# Sum of squared residuals\n",
    "ss_res = np.sum(residuals**2)\n",
    "\n",
    "# Sample variance of the residuals (mean square error)\n",
    "mse = ss_res / (len(y) - 2)\n",
    "\n",
    "# Standard error of coefficients\n",
    "X = np.vstack([x, np.ones(len(x))]).T  # Independent variable matrix with a column of ones for the intercept\n",
    "var_beta = mse * np.linalg.inv(np.dot(X.T, X)).diagonal()  # Variance of coefficients\n",
    "se_beta = np.sqrt(var_beta)  # Standard error of coefficients\n",
    "t_stats = coefficients / se_beta\n",
    "# Calculate standard error for each coefficient\n",
    "# We assume 'X' is your matrix of independent variables\n",
    "X = np.vstack([np.ones(len(x)), x]).T  # Add a column of ones for the intercept\n",
    "XtX_inv = np.linalg.inv(X.T.dot(X))  # Calculate (X^T * X)^(-1)\n",
    "mse = np.mean((y - y_pred) ** 2)  # Mean squared error\n",
    "se = np.sqrt(np.diagonal(mse * XtX_inv))  # Standard error of coefficients\n",
    "\n",
    "# Calculate t-statistics\n",
    "t_stats = coefficients / se\n",
    "\n",
    "print(\"T-statistics for the coefficients:\", t_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df510be4-4922-4b7f-9275-8168a0664cb1",
   "metadata": {},
   "source": [
    "The t-test is appropriate for hypothesis testing in regression analysis when you are trying to determine if there is a statistically significant relationship between the independent and dependent variables. It tests whether the coefficients in a regression model are significantly different from zero in a sample.\r\n",
    "\r\n",
    "However, your output indicates an R-squared value that is negative, which suggests that the model fits the data worse than a horizontal line at the mean of the dependent variable (sentPrice). This could happen when the predictions are worse than just predicting the mean value of sentPrice, and it implies that the model might not be appropriate foourur dat\n",
    "a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c519d5e-cac9-4b6c-8849-86d79183f30e",
   "metadata": {},
   "source": [
    "For the slope (1.5945055): If this t-statistic corresponds to a p-value below a certain significance level (commonly 0.05), it indicates that the slope is significantly different from zero. The value of 1.59 is on the lower side for significance, and without the corresponding p-value or critical t-value, it’s not possible to definitively say whether it's significant.\r\n",
    "\r\n",
    "For the intercept (52.64044185): This high t-statistic suggests the intercept is significantly different from zero, implying that the baseline level of sentPrice when the similarity_bin is zero is significantly above zero.\r\n",
    "\r\n",
    "Given that the R-squared value is effectively zero (or negative), even if the t-statistics indicate that coefficients are statistically significant, the model explains none of the variability of the dependent variable around its mean. This model is not useful for predictive purposes or possibly even for inference about the relationship between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e572f59-68a8-44a4-a109-fbfa3d8405c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets fit our Model with NumPy:\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "y_pred = X @ beta\n",
    "residuals = y - y_pred\n",
    "sigma_squared = np.sum(residuals**2) / (X.shape[0] - X.shape[1])\n",
    "cov_matrix = sigma_squared * np.linalg.inv(X.T @ X)\n",
    "std_errors = np.sqrt(np.diag(cov_matrix))\n",
    "t_stats = beta / std_errors\n",
    "alpha = 0.05\n",
    "t_critical_approx = 1.96  # Approximation for large df\n",
    "conf_intervals = [(b - t_critical_approx * se, b + t_critical_approx * se) for b, se in zip(beta, std_errors)]\n",
    "\n",
    "print(\"Coefficients (beta):\", beta)\n",
    "print(\"\\nStandard Errors:\", std_errors)\n",
    "print(\"\\nT-Statistics:\", t_stats)\n",
    "\n",
    "# Printing confidence intervals\n",
    "print(\"\\n95% Confidence Intervals:\")\n",
    "for idx, ci in enumerate(conf_intervals):\n",
    "    print(f\"Variable {idx}: {ci}\")\n",
    "\n",
    "# Note on the critical value approximation\n",
    "print(f\"\\nNote: Used a critical value approximation of {t_critical_approx} for large degrees of freedom.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a83da-9592-4b77-94c3-b5a8001a0051",
   "metadata": {},
   "source": [
    "In our scenario, we conducted a linear regression analysis to investigate the hypothesis that apps with similar descriptions, as represented by their embeddings, would have similar asking prices in auctions. After calculating the regression coefficients (beta), standard errors, t-statistics, and confidence intervals using NumPy, here's what we found:\r\n",
    "\r\n",
    "Coefficients (Beta): We obtained two coefficients: [1.1407896, 0.06927775]. The first coefficient corresponds to the intercept, and the second to the slope of our regression line. These coefficients suggest a relationship between the average similarity of app descriptions and their asking prices in auctions.\r\n",
    "\r\n",
    "Standard Errors: The standard errors for these coefficients are [0.0591484, 0.01751689]. These values help us understand the precision of our coefficient estimates. Lower standard errors indicate more precise estimates.\r\n",
    "\r\n",
    "T-Statistics: The t-statistics [19.28690588, 3.95491223] indicate how many standard deviations our coefficients are from 0. These values are used to assess the statistical significance of the coefficients.\r\n",
    "\r\n",
    "95% Confidence Intervals:\r\n",
    "\r\n",
    "For the intercept: (1.024858735445414, 1.2567204579809839)\r\n",
    "For the slope: (0.03494465353206231, 0.10361085339321854)\r\n",
    "These intervals give us a range within which we can be 95% confident that the true value of our coefficients lies.\r\n",
    "\r\n",
    "Given the t-statistics and confidence intervals, we observe that both coefficients seem significant, indicating a relationship between the similarity of app descriptions and their asking prices. However, it's crucial to note that these results are based on an approximation using a critical value of 1.96, typically applied when degrees of freedom are large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2663f145-186e-4574-9397-4017a32e55b4",
   "metadata": {},
   "source": [
    "To further validate our findings and explore the relationships in our data, we could consider:\n",
    "\n",
    "Cross-validation: Split our data into training and test sets to evaluate the model's performance on unseen data. This can help us assess the model's predictive accuracy and generalizability.\n",
    "\n",
    "Exploring Additional Features: Our current model focuses on the similarity of app descriptions. Incorporating other features from the auctions table (like CountryCode and OS) might provide a more comprehensive understanding of what influences asking prices.\n",
    "\n",
    "Interaction Effects: Examining interaction terms between app description similarity and other variables (e.g., OS version or country) might reveal more complex relationships affecting auction prices.\n",
    "\n",
    "Model Complexity: If our initial model doesn't capture the nuances of our data well, we might explore more complex models or non-linear relationships. Polynomial regression or even machine learning models could offer deeper insights, provided we carefully manage model complexity to avoid overfitting.\n",
    "\n",
    "Statistical Tests for Categorical Variables: If we incorporate categorical variables like OS, using ANOVA or similar statistical tests could help assess the overall significance of these categories on auction prices, complementing the individual coefficient analysis from regression.\n",
    "\n",
    "By following these steps, we aim to refine our analysis, potentially uncovering more nuanced insights into the factors that influence the auction prices of mobile app advertisements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7fc6f4-90c0-4f74-89a2-c1618b49654f",
   "metadata": {},
   "source": [
    "lets start with cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae40784-88e9-42ba-8b90-eeb05f0347b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Number of observations\n",
    "n = X.shape[0]\n",
    "\n",
    "# Shuffle the dataset (optional but recommended)\n",
    "indices = np.arange(n)\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "split_ratio = 0.8  # 80% of the data used for training, 20% for testing\n",
    "split_index = int(n * split_ratio)\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Fit the model on the training data using pseudoinverse\n",
    "beta = np.linalg.pinv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = X_test @ beta\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE) on the test set\n",
    "mse = np.mean((y_test - y_pred) ** 2)\n",
    "# R-squared calculation\n",
    "ss_res = np.sum((y_test - y_pred) ** 2)\n",
    "ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "# MAE calculation\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "\n",
    "print(f\"R-squared on Test Set: {r_squared}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Test Set: {mae}\")\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf37ad-3765-43e9-a62b-f735dab264df",
   "metadata": {},
   "source": [
    "The results from our model evaluation on the test set offer insights into its performance and predictive accuracy:\r\n",
    "\r\n",
    "R-squared on Test Set: 0.00021489117968065408: This value is very close to zero, indicating that the model explains a very small fraction of the variance in the dependent variable (auction prices) based on the independent variables (features like app description similarity, OS version, etc.) in the test set. Essentially, this suggests that the model, as currently specified, has limited predictive power and does not capture the underlying relationship between the features and auction prices effectively.\r\n",
    "\r\n",
    "Mean Absolute Error (MAE) on Test Set: 1.911010887864504: The MAE provides an average of the absolute differences between predicted and actual auction prices. A MAE of approximately 1.91 means that on average, the model's predictions deviate from the actual prices by about 1.91 units. The scale and acceptability of this error depend on the context of your problem and the range of sentPrice in your dataset.\r\n",
    "\r\n",
    "Mean Squared Error (MSE) on Test Set: 32.726211377151074: The MSE is higher than the MAE, as expected, because it squares the errors before averaging them, thus giving a larger weight to larger errors. An MSE of approximately 32.73 suggests that there are significant deviations between the predicted and actual prices, reinforced by the fact that squaring the errors amplifies the impact of larger errors on this metric.\r\n",
    "\r\n",
    "For us, these metrics indicate that while we have developed a regression model to predict auction prices based on various features, the model's current formulation does not strongly predict auction prices. The very low R-squared value suggests that other unaccounted factors may be influencing auction prices, or that the relationships between the features and auction prices are more complex than our model captures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab6c81-49ce-4558-a39a-db0a717afcc7",
   "metadata": {},
   "source": [
    "Now, let's address the second question about examining interaction terms, specifically in the context of \"app description similarity and other variables (e.g., OS version or country)\" to uncover complex relationships that might affect auction prices. To include interaction terms in your regression model, you'd manually create these terms and add them to your feature matrix. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2880b77-e51c-4df8-9caa-39452c7e0e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's create a simple manual label encoder for 'os_version'\n",
    "unique_os_versions = df['osAndVersion'].unique()\n",
    "os_version_to_int = {key: value for value, key in enumerate(unique_os_versions)}\n",
    "\n",
    "# Map the categorical 'os_version' to integers\n",
    "df['os_version_encoded'] = df['osAndVersion'].map(os_version_to_int)\n",
    "\n",
    "# Now, 'df' contains a new column 'os_version_encoded' with numeric representations of the OS versions\n",
    "os_version_encoded= df['os_version_encoded']\n",
    "# Manually create an interaction term\n",
    "interaction_term = similarity * os_version_encoded\n",
    "\n",
    "# Convert the interaction_term pandas Series to a numpy array and reshape\n",
    "interaction_term_array = interaction_term.values.reshape(-1, 1)\n",
    "\n",
    "# X = np.hstack([X, interaction_term_array])\n",
    "# beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "# y_pred = X @ beta\n",
    "# residuals = y - y_pred\n",
    "# y_pred = X @ beta\n",
    "# SS_res = np.sum(residuals**2)\n",
    "# SS_tot = np.sum((y - np.mean(y))**2)\n",
    "# R_squared = 1 - (SS_res / SS_tot)\n",
    "# print(f\"R-squared: {R_squared}\")\n",
    "\n",
    "# Replace the direct inversion and matrix multiplication with numpy.linalg.lstsq\n",
    "# X = np.hstack([X, interaction_term_array])  # Assuming this is correctly shaped\n",
    "# Add a column of ones if you haven't already to include the intercept\n",
    "X_with_intercept = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Use np.linalg.lstsq to find the beta coefficients\n",
    "beta, residuals, rank, s = np.linalg.lstsq(X_with_intercept, y, rcond=None)\n",
    "\n",
    "# Predict y using the beta coefficients\n",
    "y_pred = X_with_intercept @ beta\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y - y_pred\n",
    "\n",
    "# Now, you can proceed with further analysis like calculating R-squared\n",
    "RSS = np.sum(residuals**2)  # Residual Sum of Squares\n",
    "TSS = np.sum((y - np.mean(y))**2)  # Total Sum of Squares\n",
    "R_squared = 1 - RSS / TSS\n",
    "\n",
    "print(f\"R-squared: {R_squared}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17d796-71af-4b97-9085-8f1adc8cd9b7",
   "metadata": {},
   "source": [
    "Receiving an R-squared value of approximately 0.0004 indicates that the model, including the interaction term you've added, explains a very small fraction of the variance in the dependent variable (the auction price in this context). Here's what this could mean in the context of our data and steps we've taken:\n",
    "\n",
    "Interpretation:\n",
    "Minimal Variance Explained: The R-squared value suggests that the combination of features and the interaction term you've included in the model does not significantly explain the variability in auction prices. Essentially, the model is only slightly better than a simple model that always predicts the mean auction price, regardless of the input features.\n",
    "Potential Overfitting: While the R-squared is low here, in cases where it's significantly higher, one should also consider the possibility of overfitting, especially when adding interaction terms or many predictors to a model. However, in this case, the concern is not overfitting but underfitting, where the model fails to capture the underlying relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb589e6-2ea9-4c68-9627-b272ce51f85a",
   "metadata": {},
   "source": [
    "Model Complexity with Polynomial Regression\n",
    "Polynomial regression allows you to model non-linear relationships between the independent variables and the dependent variable by introducing polynomial terms (squared, cubed, etc.) of the independent variables. This can capture more complex patterns in your data but also increases the risk of overfitting, especially as the degree of the polynomial increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3560c-cea9-4766-a1c2-23930f16affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's create a 2nd degree polynomial feature for demonstration\n",
    "x_squared = x ** 2\n",
    "\n",
    "# Now, prepare the design matrix X with the original and the squared term\n",
    "X_poly = np.vstack([np.ones(len(x)), x, x_squared]).T\n",
    "\n",
    "# Fit the polynomial regression model\n",
    "beta_poly = np.linalg.inv(X_poly.T @ X_poly) @ X_poly.T @ y\n",
    "\n",
    "# Predictions\n",
    "y_pred_poly = X_poly @ beta_poly\n",
    "\n",
    "# Calculate residuals\n",
    "residuals_poly = y - y_pred_poly\n",
    "\n",
    "# Calculate the Total Sum of Squares (TSS)\n",
    "TSS = np.sum((y - np.mean(y))**2)\n",
    "\n",
    "# Calculate the Residual Sum of Squares (RSS)\n",
    "RSS = np.sum(residuals_poly**2)\n",
    "\n",
    "# Calculate R-squared\n",
    "R_squared_poly = 1 - (RSS / TSS)\n",
    "\n",
    "# Printing the results\n",
    "print(f\"R-squared for Polynomial Regression Model: {R_squared_poly:.3f}\")\n",
    "print(f\"Residuals: {residuals_poly[:10]}\")  # Printing the first 10 residuals as an example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d5d6e-d4e2-4c15-8a30-07fd01655ac1",
   "metadata": {},
   "source": [
    "R-squared for Polynomial Regression Model: 0.000: This indicates that the polynomial regression model explains none of the variance in the dependent variable around its mean. An R-squared value of 0 suggests that the model does not improve the prediction over simply using the mean of the dependent variable as the prediction for all observations. In practical terms, this means the model, as currently specified, likely does not capture the relationship between your predictors and the outcome effectively.\n",
    "\n",
    "Residuals: The list of residuals represents the difference between the actual values of your dependent variable and the values predicted by your model for the first 10 observations. For example, a residual of -1.30098274 for the first observation indicates that the model's prediction for this particular data point was higher than the actual value by approximately 1.3 units.\n",
    "\n",
    "Interpreting These Results\n",
    "Low Predictive Power: The near-zero R-squared value suggests that the model's predictive power is minimal, and it might not be capturing the necessary dynamics of the underlying data. This could be due to several reasons, such as not including relevant variables, needing higher-order polynomial terms (if underfitting), or the data inherently lacking a polynomial relationship.\n",
    "\n",
    "About the Residuals: The residuals show how off the predictions are for individual data points. Positive values indicate underestimations by the model, while negative values indicate overestimations. The variability in the residuals you provided (-1.3 to 4.05) highlights inconsistencies in the model's prediction accuracy across different observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2a038-67f7-477f-830d-279d2d7c5ce4",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) is a statistical method used to compare the means of three or more samples. For regression, ANOVA can test whether the means of different groups (defined by a categorical variable) differ significantly. This is useful for assessing the overall impact of categorical variables like OS on our dependent variable (auction prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2b004-5b5d-4180-adab-cef7eeac4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example data (Replace with your actual data frame)\n",
    "os_groups = df['osAndVersion'].astype('category').cat.codes.values\n",
    "auction_prices = df['sentPrice'].values\n",
    "# auction_prices = df['bidFloorPrice'].values\n",
    "# Calculate group means\n",
    "unique_groups = np.unique(os_groups)\n",
    "group_means = {group: auction_prices[os_groups == group].mean() for group in unique_groups}\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = auction_prices.mean()\n",
    "\n",
    "# Calculate Between-Group Sum of Squares (SSB)\n",
    "SSB = sum(len(auction_prices[os_groups == group]) * (mean - overall_mean) ** 2 for group, mean in group_means.items())\n",
    "\n",
    "# Calculate Within-Group Sum of Squares (SSW)\n",
    "SSW = sum(sum((auction_prices[os_groups == group] - mean) ** 2) for group, mean in group_means.items())\n",
    "\n",
    "# Calculate total sum of squares (SST)\n",
    "SST = sum((auction_prices - overall_mean) ** 2)\n",
    "\n",
    "# Degrees of freedom\n",
    "df_between = len(unique_groups) - 1\n",
    "df_within = len(auction_prices) - len(unique_groups)\n",
    "\n",
    "# Mean Squares\n",
    "MSB = SSB / df_between\n",
    "MSW = SSW / df_within\n",
    "\n",
    "# F-statistic\n",
    "F = MSB / MSW\n",
    "\n",
    "print(f\"F-statistic: {F}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f21a5-af62-4f98-99ec-031284854c77",
   "metadata": {},
   "source": [
    "We processed our dataset to investigate the impact of operating system (OS) versions on auction prices, employing a simplified ANOVA approach using numpy. Here's a summary of our steps and findings:\n",
    "\n",
    "Data Preparation: We started by encoding the OS categorical variable into numeric group identifiers, allowing us to handle the data more effectively in a numerical context. This encoding transformed the OS categories into a format suitable for statistical analysis.\n",
    "\n",
    "Group Mean Calculation: For each unique OS group, we calculated the mean auction price, setting the stage for comparing these group means against the overall mean auction price. This step was crucial for identifying potential variances in auction prices across different OS versions.\n",
    "\n",
    "Variance Analysis: We decomposed the total variance in auction prices into between-group and within-group components. This allowed us to assess whether the differences in mean auction prices between OS groups were significant compared to the variation within those groups.\n",
    "\n",
    "F-Statistic Calculation: We computed the F-statistic as 12.44, which quantifies the ratio of variance between the OS groups to the variance within the OS groups. A higher F-statistic suggests significant differences between the group means.\n",
    "\n",
    "Interpretation: Our calculated F-statistic indicates that there are statistically significant differences in auction prices among different OS versions. This suggests that the OS version has a measurable impact on auction prices, with some OS versions correlating with higher or lower prices than others.\n",
    "\n",
    "Considerations: While this analysis provided valuable insights, it's important to remember that our approach was simplified. A more comprehensive analysis might include checks for the assumptions underlying ANOVA, explore other influencing factors, or utilize advanced statistical software for a more nuanced understanding.\n",
    "\n",
    "Next Steps: Based on our findings, we might consider further investigating how specific OS versions influence auction prices or explore additional variables that could affect auction outcomes. Incorporating more complex models or utilizing specialized statistical software could enhance our analysis and provide deeper insights.\n",
    "Having identified significant differences among OS version groups in their impact on auction prices through our ANOVA analysis, we now consider the following next steps in our investigation:\r\n",
    "\r\n",
    "Post hoc Testing: Our ANOVA results suggest significant variances among the different OS version groups. To further understand these differences, we are interested in conducting post hoc tests, such as Tukey's HSD. This step would help us pinpoint which specific OS versions differ significantly from each other in terms of auction prices. However, implementing post hoc tests like Tukey's HSD manually in a numpy-only environment is complex and may not be straightforward without the functionalities provided by libraries like scipy or statistical software that readily offer these tests.\r\n",
    "\r\n",
    "Checking ANOVA Assumptions: A foundational part of our analysis involves ensuring that the assumptions underpinning ANOVA are satisfied. These include the normality of residuals and the homogeneity of variances among the groups. Verifying these assumptions is critical; failure to meet them may lead us to consider alternative analytical methods or data transformations to uphold the validity of our conclusions. Directly assessing these assumptions requires statistical tests and visualizations that are challenging to implement accurately with basic tools, underscoring the value of specialized statistical libraries.\r\n",
    "\r\n",
    "Further Modeling: We are also contemplating the exploration of interaction effects and the development of different models for data subsets. Such efforts aim to uncover more nuanced insights into how various factors, in combination with OS versions, influence auction prices. This exploration might involve more complex statistical or machine learning models, which could reveal deeper patterns and relationships within our data.\r\n",
    "\r\n",
    "Utilization of Advanced Tools: While our manual approach to calculating the F-statistic has been insightful, we recognize the limitations of not using comprehensive statistical software or libraries like statsmodels. For more detailed analyses that include automatic checks for ANOVA assumptions, calculation of p-values, and execution of post hoc tests, leveraging these advanced tools is essential. They not only simplify the process but also enhance the reliability and depth of our findings.\r\n",
    "\r\n",
    "In summary, our next steps involve a careful consideration of post hoc analyses to understand specific group differences, a thorough check of the ANOVA assumptions to ensure the robustness of our findings, and further exploratory modeling to delve deeper into the data's complexities. We acknowledge the limitations posed by a numpy-only approach for these advanced analyses and recognize the value of specialized statistical libraries in supporting our ongoing research efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2433d0-016a-43c0-b79b-c040c148a330",
   "metadata": {},
   "source": [
    "when we calculate an F-statistic of 4.570967425758933 using sentPrice instead of bidFloorPrice, it indicates the result of an ANOVA test where osAndVersion groups are used to predict variations in sentPrice. Here's what the components and the result mean in this context:\n",
    "\n",
    "Overall Mean: The average of sentPrice across all observations, serving as a reference point to compare each group's mean against.\n",
    "Group Means: The average sentPrice within each OS version group. These are compared to the overall mean to assess whether significant differences exist between groups.\n",
    "Between-Group Sum of Squares (SSB): Reflects the variability due to the interaction between the different OS versions. A higher SSB indicates more variability between groups, suggesting that different OS versions might have different average sentPrice values.\n",
    "Within-Group Sum of Squares (SSW): Captures the variability within each OS version group. If the groups are very different in terms of sentPrice, but there's also a lot of variability within groups, it might be harder to attribute differences directly to the OS version.\n",
    "Total Sum of Squares (SST): The total variability in the dataset regarding sentPrice.\n",
    "Degrees of Freedom: df_between represents the number of groups minus one, and df_within is the total number of observations minus the number of groups. These help in adjusting the sums of squares to account for the size of the dataset and the number of groups.\n",
    "Mean Squares (MSB and MSW): Average variability between groups and within groups, respectively. These are calculated by dividing the sums of squares by their corresponding degrees of freedom.\n",
    "F-statistic: The ratio of MSB to MSW. An F-statistic greater than 1 suggests that there is more variability between groups than within groups, indicating that the OS versions might affect sentPrice.\n",
    "In this specific case, an F-statistic of 4.570967425758933 suggests that there are significant differences in sentPrice across different OS versions. However, to understand which specific groups (OS versions) differ from each other, post hoc tests like Tukey's HSD would be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40e39d-7f86-46c5-b9ba-51356095c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardize 'sentPrice' and 'bidFloorPrice'\n",
    "sentPrice_standardized = (df['sentPrice'] - np.mean(df['sentPrice'])) / np.std(df['sentPrice'])\n",
    "bidFloorPrice_standardized = (df['bidFloorPrice'] - np.mean(df['bidFloorPrice'])) / np.std(df['bidFloorPrice'])\n",
    "#  One-hot encoding for categorical variable 'osAndVersion'\n",
    "os_encoded = pd.get_dummies(df['osAndVersion'], drop_first=True).values\n",
    "\n",
    "# Calculate overall mean of 'sentPrice'\n",
    "overall_mean = np.mean(df['sentPrice'].values)\n",
    "\n",
    "# SSW and SSB calculation for a single categorical variable (e.g., 'osAndVersion')\n",
    "SSW = 0\n",
    "SSB = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac0363a-8405-49ef-8900-46beb1b8e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the variable name to use 'sentPrice_standardized'\n",
    "for category in range(os_encoded.shape[1]):  # For each category in 'osAndVersion'\n",
    "    category_data = sentPrice_standardized[os_encoded[:, category] == 1]\n",
    "    category_mean = np.mean(category_data)\n",
    "    SSW += np.sum((category_data - category_mean) ** 2)\n",
    "    SSB += len(category_data) * (category_mean - overall_mean) ** 2\n",
    "# Assuming these variables are defined elsewhere in your code\n",
    "df_between = os_encoded.shape[1] - 1\n",
    "df_within = len(sentPrice_standardized) - os_encoded.shape[1]\n",
    "MSB = SSB / df_between\n",
    "MSW = SSW / df_within\n",
    "F = MSB / MSW\n",
    "\n",
    "print(f\"F-statistic for OS and Version: {F}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f81cbe-6663-4a90-bc95-93f178f808e5",
   "metadata": {},
   "source": [
    "An F-statistic of 674.1981864061144 is quite large and typically suggests that there's a statistically significant difference between the group means being analyzed. In the context of your analysis on the impact of the operating system version (osAndVersion) on the standardized auction prices (sentPrice), this F-statistic would indicate a strong relationship between the OS version and the auction prices.\r\n",
    "\r\n",
    "Such a high F-statistic suggests that at least one OS version group has a mean significantly different from the others when it comes to auction prices. This could be an indication that certain OS versions are associated with higher or lower auction prices compared to the overall average.\r\n",
    "\r\n",
    "However, to fully interpret the F-statistic, you would also need to consider the degrees of freedom associated with the numerator (between groups) and the denominator (within groups), as well as the p-value associated with the F-statistic. The p-value would tell you the probability of observing such a large F-statistic if in reality, there was no difference between the group means (null hypothesis). A commonly used threshold for statistical significance is a p-value of less than 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9a4a4-467d-4afc-b5cc-2e64a25b43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encoding for categorical variable 'osAndVersion'\n",
    "os_encoded = pd.get_dummies(df['osAndVersion'], drop_first=True).values\n",
    "# 'sentPrice' data\n",
    "sentPrice = df['sentPrice'].values\n",
    "# 'bidFloorPrice' data\n",
    "bidFloorPrice = df['bidFloorPrice'].values\n",
    "\n",
    "# Number of categories in 'osAndVersion' (after one-hot encoding and dropping first category)\n",
    "num_categories = os_encoded.shape[1] \n",
    "\n",
    "# Number of observations\n",
    "n_obs = len(sentPrice) \n",
    "\n",
    "# Degrees of freedom between (df_between)\n",
    "df_between = num_categories - 1\n",
    "\n",
    "# Degrees of freedom within (df_within)\n",
    "df_within = n_obs - num_categories\n",
    "\n",
    "# Total degrees of freedom\n",
    "df_total = n_obs - 1\n",
    "\n",
    "print(f\"Degrees of Freedom Between: {df_between}\")\n",
    "print(f\"Degrees of Freedom Within: {df_within}\")\n",
    "print(f\"Total Degrees of Freedom: {df_total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e2d87-1f30-472b-833b-6dc0c9a4008b",
   "metadata": {},
   "source": [
    "calculationg the exsexct p value without scipy is quite complex but for this task I thnk we can a conceptual example that may not perform well due to the complexity of accurately calculating F-distribution properties through numerical methods. \n",
    "Real-world applications should rely on established libraries like scipy.stats for such tasks, which offer functions like scipy.stats.f.cdf to directly compute p-values from F-statistics efficiently and accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db60ad-ec71-49df-8ec3-9da58cf94442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def f_pdf(x, dfn, dfd):\n",
    "    \"\"\"\n",
    "    Probability density function of the F-distribution.\n",
    "    Attempting a more stable computation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Adjusted to use the 'math' module and attempting to stabilize with logs\n",
    "        log_numerator = math.log(math.gamma((dfn + dfd) / 2)) + ((dfn / 2) - 1) * math.log(dfn / dfd) + ((dfn / 2) - 1) * math.log(x)\n",
    "        log_denominator = math.log(math.gamma(dfn / 2)) + math.log(math.gamma(dfd / 2)) + ((dfn + dfd) / 2) * math.log(1 + (dfn / dfd) * x)\n",
    "        return np.exp(log_numerator - log_denominator)\n",
    "    except OverflowError:\n",
    "        # If overflow, return a large number; this is not accurate but allows the code to continue.\n",
    "        return float('inf')\n",
    "\n",
    "def trapezoidal_rule(f, a, b, n, **kwargs):\n",
    "    \"\"\"\n",
    "    Numerically approximate the integral of f from a to b using the trapezoidal rule with n intervals.\n",
    "    \"\"\"\n",
    "    x = np.linspace(a, b, n+1)\n",
    "    y = np.array([f(xi, **kwargs) for xi in x])\n",
    "    h = (b - a) / n\n",
    "    return (h / 2) * np.sum(y[:-1] + y[1:])\n",
    "\n",
    "def f_cdf(f, dfn, dfd):\n",
    "    \"\"\"\n",
    "    Approximate the CDF of the F-distribution at point f for dfn and dfd degrees of freedom using numerical integration.\n",
    "    \"\"\"\n",
    "    # Integration bounds and intervals\n",
    "    a, b, n = 0, f, 10000  # Adjust 'a' and 'n' based on desired accuracy and computational limitations\n",
    "    return trapezoidal_rule(f_pdf, a, b, n, dfn=dfn, dfd=dfd)\n",
    "\n",
    "\n",
    "dfn = df_between  # Degrees of freedom numerator (between-groups)\n",
    "dfd =df_within # Degrees of freedom denominator (within-groups)\n",
    "f_statistic = F  # Example F-statistic\n",
    "\n",
    "# Calculate the p-value (right-tail)\n",
    "p_value = 1 - f_cdf(f_statistic, dfn, dfd)\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0f86d-7373-4a81-a7e9-30abef5f0339",
   "metadata": {},
   "source": [
    "The result \"P-value: -inf\" indicates that the approach to compute the p-value for the F-statistic using numerical integration has encountered numerical stability issues, resulting in an overflow that leads to an infinite negative value. This outcome highlights the challenges of manually calculating the cumulative distribution function (CDF) of the F-distribution for extreme values or large datasets without using specialized numerical libraries designed to handle such computations. the original code I tried :\n",
    "def f_pdf(x, dfn, dfd):\r\n",
    "    \"\"\"\r\n",
    "    Probability density function of the F-distribution.\r\n",
    "    \"\"\"\r\n",
    "    numerator = (np.math.gamma((dfn + dfd) / 2) * (dfn / dfd) ** (dfn / 2) * x ** ((dfn / 2) - 1))\r\n",
    "    denominator = (np.math.gamma(dfn / 2) * np.math.gamma(dfd / 2) * (1 + (dfn / dfd) * x) ** ((dfn + dfd) / 2))\r\n",
    "    return numerator / denominator\r\n",
    "\r\n",
    "def f_cdf(f, dfn, dfd):\r\n",
    "    \"\"\"\r\n",
    "    Numerically approximate the CDF of the F-distribution for given F-statistic and degrees of freedom.\r\n",
    "    \"\"\"\r\n",
    "    # Define the trapezoidal rule function for numerical integration\r\n",
    "    def trapezoidal_rule(f, a, b, n, **kwargs):\r\n",
    "        x = np.linspace(a, b, n+1)\r\n",
    "        y = f(x, **kwargs)\r\n",
    "        h = (b - a) / n\r\n",
    "        return (h / 2) * np.sum(y[:-1] + y[1:])\r\n",
    "    \r\n",
    "    # Integration bounds and intervals\r\n",
    "    a, b, n = 0, f, 10000  # Adjust 'b' and 'n' as needed\r\n",
    "    return trapezoidal_rule(f_pdf, a, b,\n",
    "resulted in he OverflowError typically happens when the numbers involved in the calculation exceed the maximum limit that Python's floating-point arithmetic can handle. sadly as for submiting I couldn't find a way to fix on problem without getting one error for another.. so I want atempt to reimplament Scipy in the seemly close future =] n, dfn=dfn, dfd=dfd)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b5372a-9b2c-4b6e-852b-d1b53eabdf48",
   "metadata": {},
   "source": [
    "The analysis conducted in this notebook provides insightful revelations into the dynamics influencing auction prices within mobile app advertising. Through rigorous statistical exploration, including regression models and ANOVA, we uncovered that app description similarities have a marginal impact on auction prices, as evidenced by R-squared values close to zero. This outcome suggests that the predictive capability of app description similarity on auction prices is minimal. Further, the ANOVA analysis highlighted significant variances in auction prices across different OS versions, pointing towards the operating system as a noteworthy factor in auction price determination.\r\n",
    "\r\n",
    "Despite these insights, the overall low explanatory power of the developed models underscores the presence of other, unaccounted-for factors or more complex relationships that influence auction prices beyond the scope of app descriptions and OS versions. The findings invite additional inquiries into more variables, sophisticated modeling techniques, and deeper analysis of interactions among the factors at play.\r\n",
    "\r\n",
    "In conclusion, while the initial hypotheses about the impact of app descriptions and OS versions on auction prices were partially supported, the limited model effectiveness signals a need for broader investigation. The pursuit of understanding the full spectrum of influences on auction prices remains open, suggesting avenues for future research to refine predictive models, incorporate a wider array of variables, and ultimately, unearth more comprehensive insights into the factors driving auction prices in the mobile app ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14da521-99eb-47a6-99f9-12c33cb986db",
   "metadata": {},
   "source": [
    "As suggested erlier , I will now try to look for other variables - Exploring the impact of the country as an independent variable on auction prices can provide valuable insights into geographical influences on market dynamics. \n",
    "Here's a suggested approach for conducting this analysis that I would do if I had non limited libary acsess:\n",
    "\n",
    "\n",
    "Start with descriptive statistics of auction prices for different countries. Calculate means, medians, and standard deviations to get an initial sense of how auction prices vary by country.\n",
    "Visual Exploration:\n",
    "\n",
    "Use visualizations such as box plots or histograms to compare auction prices across different countries. This can help identify patterns, outliers, or any country-specific trends in auction prices.\n",
    "Encoding Categorical Data:\n",
    "\n",
    "Since 'country' is a categorical variable, I'll need to convert it into a format suitable for regression analysis. One-hot encoding is a common approach that transforms categorical data into binary columns, one for each country.\n",
    "Regression Analysis:\n",
    "\n",
    "Incorporate the encoded country data into your regression model as independent variables. This will allow us to assess the impact of each country on auction prices while controlling for other factors.\n",
    "we can consider using both simple linear regression models to assess individual effects and multiple regression models to understand the combined impact of country and other variables on auction prices.\n",
    "ANOVA for Country Groups:\n",
    "\n",
    "Conduct an Analysis of Variance (ANOVA) to test if there are significant differences in auction prices between countries. This can complement your regression analysis by providing a statistical test for the impact of country as a categorical variable.\n",
    "Interaction Terms:\n",
    "\n",
    "Explore interaction terms between 'country' and other variables, such as OS version or app description similarity. This can reveal if the impact of one variable on auction prices depends on the level of another variable.\n",
    "Post-hoc Analysis:\n",
    "\n",
    "If ANOVA indicates significant differences, perform post-hoc tests to identify which specific country pairs differ. This step helps in understanding the pairwise differences highlighted by ANOVA.\n",
    "Model Evaluation and Validation:\n",
    "\n",
    "Evaluate the performance of your models using appropriate metrics such as R-squared for regression models and the F-statistic for ANOVA. Additionally, consider cross-validation techniques to assess the model's predictive accuracy and generalizability.\n",
    "Sensitivity Analysis:\n",
    "\n",
    "Conduct sensitivity analysis to understand the robustness of your findings. This could involve varying the model specifications or excluding potentially influential outliers.\n",
    "Further Exploration:\n",
    "\n",
    "Based on the results, consider exploring additional variables that might interact with the country, such as economic indicators or mobile usage patterns, to deepen your understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcc79f-6cf3-4b55-b93c-531cbd0d93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and it has columns 'country' and 'sentPrice'\n",
    "grouped = df.groupby('countryCode')['sentPrice']\n",
    "\n",
    "# Calculate means, medians, and standard deviations for each country\n",
    "# Calculate means, medians, standard deviations, and counts for each country\n",
    "means = grouped.mean()\n",
    "medians = grouped.median()\n",
    "std_devs = grouped.std()\n",
    "counts = grouped.count()  # Add count for the number of auctions per country\n",
    "\n",
    "# Combine these series into a DataFrame for a comprehensive view\n",
    "descriptive_stats = pd.DataFrame({\n",
    "    'Mean': means,\n",
    "    'Median': medians,\n",
    "    'Standard Deviation': std_devs,\n",
    "    'Count': counts  # Include the count in your descriptive statistics\n",
    "})\n",
    "\n",
    "# Sort or filter the DataFrame based on 'Count' if needed\n",
    "# descriptive_stats = descriptive_stats.sort_values(by='Count', ascending=False)\n",
    "\n",
    "print(descriptive_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d0c3a-a3f8-4070-bacd-db0d87305864",
   "metadata": {},
   "source": [
    "aving many countries with only one data point and consequently receiving NaN for the standard deviation (and possibly other statistics) can indeed impact the models and analyses in several ways:\n",
    "\n",
    "Descriptive Statistics\n",
    "For descriptive statistics, standard deviation NaN values indicate insufficient data to describe variability. In this context, it means that any measure of dispersion or variability for those countries is undefined or not reliable.\n",
    "\n",
    "Regression Analysis\n",
    "In regression analysis, especially if you're using one-hot encoding for countries, countries with very few data points can lead to issues like:\n",
    "\n",
    "Overfitting: The model might fit too closely to the few data points available for some countries, capturing noise rather than the underlying data pattern. This reduces the model's generalizability.\n",
    "Coefficient Instability: Regression coefficients associated with countries having very few observations might be unstable and unreliable. Small changes in the data could lead to large changes in the coefficient estimates.\n",
    "Predictive Power: The model’s ability to predict auction prices for countries with sparse data might be poor due to the lack of sufficient training data for those categories.\n",
    "ANOVA\n",
    "For ANOVA, having groups (in this case, countries) with only one observation can be problematic because:\n",
    "\n",
    "ANOVA Assumptions: One of the assumptions of ANOVA is that the groups being compared should have approximately equal variances. With only one data point per country for several countries, it's impossible to calculate or compare variances meaningfully.\n",
    "Statistical Significance Testing: ANOVA tests for differences among group means. If many groups have only a single observation, it limits the test's ability to detect significant differences due to a lack of within-group variability information.\n",
    "Interaction Terms\n",
    "When considering interaction terms involving countries, the concerns mentioned above are compounded. Interaction terms can increase model complexity, exacerbating issues with overfitting and making interpretation more challenging, especially with sparse data for many countries.\n",
    "\n",
    "Mitigation Strategies\n",
    "To mitigate these issues, you might consider:\n",
    "\n",
    "Aggregating Sparse Categories: Combine countries with very few data points into a broader category, such as \"Other.\"\n",
    "Focusing on Well-Represented Countries: Limit your analysis to countries with a sufficient number of observations to ensure statistical reliability.\n",
    "Regularization Techniques: For regression models, consider using regularization (like Lasso or Ridge regression) to handle overfitting and stabilize coefficient estimates.\n",
    "In summary, while NaN for standard deviation directly reflects on descriptive statistics, it also flags potential concerns for more complex models. Careful preprocessing and model choice can help address these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606653b-c80b-42f5-b7d0-662dd73581c0",
   "metadata": {},
   "source": [
    "Addressing the issue of countries with only one data point in your dataset involves making trade-offs between the granularity of your analysis and the statistical robustness of your findings. Here are some approaches to consider:\n",
    "\n",
    "Removing Sparse Countries\n",
    "Pros:\n",
    "\n",
    "Simplicity: Easy to implement.\n",
    "Model Stability: Reduces the risk of overfitting and ensures that the model's findings are based on well-represented groups.\n",
    "Cons:\n",
    "\n",
    "Loss of Data: Potentially valuable information about those countries is discarded.\n",
    "Reduced Coverage: The analysis might not reflect the full diversity of the dataset, especially if many countries are removed.\n",
    "Grouping Sparse Countries into \"Other\"\n",
    "Pros:\n",
    "\n",
    "Inclusivity: Retains information from countries with sparse data by grouping them into a single category.\n",
    "Reduced Overfitting: Aggregating into an \"Other\" category helps mitigate the risk of overfitting associated with many small groups.\n",
    "Statistical Robustness: Enhances the reliability of statistical tests and models by ensuring sufficient sample sizes.\n",
    "Cons:\n",
    "\n",
    "Loss of Specificity: Specific insights about individual countries with few data points are lost.\n",
    "Assumption of Homogeneity: Assumes that all countries grouped into \"Other\" are similar enough, which might not be accurate.\n",
    "Using Hierarchical or Mixed-Effects Models\n",
    "Pros:\n",
    "\n",
    "Flexibility: Allows for modeling both fixed effects (common to all groups) and random effects (varying between groups), which can account for the variability among countries with more or fewer data points.\n",
    "Efficiency: Can handle unbalanced designs well, making efficient use of all available data without needing to discard or aggregate sparse groups.\n",
    "Cons:\n",
    "\n",
    "Complexity: More complex to set up and interpret than traditional regression models.\n",
    "Computational Intensity: Typically requires more computational resources and more sophisticated statistical software.\n",
    "Bayesian Methods\n",
    "Pros:\n",
    "\n",
    "Incorporation of Prior Knowledge: Allows for the integration of prior information or beliefs about the data, which can be particularly useful for countries with limited data.\n",
    "Flexibility in Handling Sparse Data: Can provide more robust estimates for groups with few observations by borrowing strength from the overall data distribution.\n",
    "Cons:\n",
    "\n",
    "Complexity: Requires a good understanding of Bayesian statistics and might necessitate specialized software.\n",
    "Computation Time: Often computationally intensive, especially for large datasets or complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c169d-88b9-4bf4-9b29-2d031d713e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = df.copy()\n",
    "country_counts = df['countryCode'].value_counts()\n",
    "data_removed_sparse = df[df['countryCode'].isin(country_counts[country_counts > 1].index)].copy()\n",
    "threshold = 2  # You can adjust this threshold based on your dataset\n",
    "data_grouped_other = df.copy()\n",
    "sparse_countries = country_counts[country_counts < threshold].index\n",
    "data_grouped_other['countryCode'] = df['countryCode'].apply(lambda x: 'Other' if x in sparse_countries else x)\n",
    "# This step is more about ensuring your data is ready for such models rather than creating a new data variable.\n",
    "data_for_advanced_models = df.copy()\n",
    "data_for_advanced_models['countryCode'] = data_for_advanced_models['countryCode'].astype('category').cat.codes\n",
    "\n",
    "\n",
    "# Placeholder function names for each analysis type\n",
    "def perform_descriptive_stats(data):\n",
    "    # Your code for calculating descriptive statistics goes here\n",
    "    return {\"Mean\": data['sentPrice'].mean(), \"Median\": data['sentPrice'].median()}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def perform_regression_analysis(data):\n",
    "    # Ensure 'countryCode' is numerically encoded if it isn't already\n",
    "    if data['countryCode'].dtype == 'object':\n",
    "        # Convert to categorical type then to category codes if it's still in object (string) format\n",
    "        data['countryCode_encoded'] = data['countryCode'].astype('category').cat.codes\n",
    "        X = data['countryCode_encoded'].values.reshape(-1, 1)  # Use the encoded column for X\n",
    "    else:\n",
    "        # Assume 'countryCode' is already appropriately encoded for numerical operations\n",
    "        X = data['countryCode'].values.reshape(-1, 1)\n",
    "\n",
    "    y = data['sentPrice'].values\n",
    "    \n",
    "    # Adding a column of ones to include the intercept term in the model\n",
    "    X_with_intercept = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "    \n",
    "    # Performing the linear regression using the Normal Equation\n",
    "    beta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y\n",
    "    \n",
    "    # Predicting y values using the regression coefficients\n",
    "    y_pred = X_with_intercept @ beta\n",
    "    \n",
    "    # Calculating residuals\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    # Calculating Total Sum of Squares (TSS) and Residual Sum of Squares (RSS)\n",
    "    TSS = np.sum((y - np.mean(y)) ** 2)\n",
    "    RSS = np.sum(residuals ** 2)\n",
    "    \n",
    "    # Calculating R-squared\n",
    "    R_squared = 1 - (RSS / TSS)\n",
    "    \n",
    "    return {\"R_squared\": R_squared}\n",
    "\n",
    "\n",
    "\n",
    "def perform_anova(data, group_col='countryCode', outcome_col='sentPrice'):\n",
    "    # Check if 'countryCode' needs encoding and do so if required\n",
    "    if group_col not in data.columns:\n",
    "        data['countryCode_encoded'] = data['countryCode'].astype('category').cat.codes\n",
    "        group_col = 'countryCode_encoded'\n",
    "    \n",
    "    # Proceed with ANOVA using possibly updated group_col\n",
    "    groups = np.unique(data[group_col])\n",
    "    overall_mean = np.mean(data[outcome_col])\n",
    "    \n",
    "    SSB = 0\n",
    "    SSW = 0\n",
    "    for group in groups:\n",
    "        group_data = data[data[group_col] == group][outcome_col]\n",
    "        group_mean = np.mean(group_data)\n",
    "        SSB += len(group_data) * (group_mean - overall_mean) ** 2\n",
    "        SSW += np.sum((group_data - group_mean) ** 2)\n",
    "    \n",
    "    df_between = len(groups) - 1\n",
    "    df_within = len(data) - len(groups)\n",
    "    \n",
    "    MSB = SSB / df_between\n",
    "    MSW = SSW / df_within\n",
    "    \n",
    "    F_statistic = MSB / MSW\n",
    "    \n",
    "    return {\"F_statistic\": F_statistic, \"df_between\": df_between, \"df_within\": df_within}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_versions = {\n",
    "    \"Original\": data_original,\n",
    "    \"Removed Sparse Countries\": data_removed_sparse,\n",
    "    \"Grouped Other Countries\": data_grouped_other,\n",
    "    \"For Advanced Models\": data_for_advanced_models\n",
    "}\n",
    "\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for name, data_version in data_versions.items():\n",
    "    # Perform descriptive stats\n",
    "    stats = perform_descriptive_stats(data_version)\n",
    "    \n",
    "    # Perform regression analysis\n",
    "    regression_results = perform_regression_analysis(data_version)\n",
    "    \n",
    "    # Perform ANOVA analysis, now just directly calling perform_anova without worrying about the column name\n",
    "    anova_results = perform_anova(data_version)\n",
    "    \n",
    "    # Store combined results in the dictionary\n",
    "    results[name] = {\n",
    "        \"Descriptive Stats\": stats,\n",
    "        \"Regression\": regression_results,\n",
    "        \"ANOVA\": anova_results\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# access to the results\n",
    "for version, analysis_results in results.items():\n",
    "    print(f\"Results for {version}:\")\n",
    "    for analysis_type, result in analysis_results.items():\n",
    "        print(f\"\\t{analysis_type}: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6149d0df-bf11-4a8b-b51f-03968d8505a9",
   "metadata": {},
   "source": [
    "The results from the analysis across different data versions—Original, Removed Sparse, Grouped Other, and Advanced Models—reveal some consistent and some varying trends in the auction prices based on country.\n",
    "\n",
    "Descriptive Statistics show that the mean auction price is fairly consistent across all data versions, hovering around 1.33 with a median of 0.07. This indicates that while the average price is relatively low, there's a wide range of auction prices, as suggested by the mean being significantly higher than the median.\n",
    "\n",
    "Regression Analysis yielded R-squared values around 0.01 for all data versions, suggesting that the country variable alone explains only about 1% of the variance in auction prices. This low percentage implies that while there is some relationship between country and auction prices, other factors not accounted for in this analysis likely play a significant role in determining auction prices.\n",
    "\n",
    "ANOVA Results showed F-statistics ranging from 8.65 to 9.99 across the data versions, with the Removed Sparse and Grouped Other data versions showing slightly higher F-statistics than the Original and Advanced Models versions. The F-statistic measures the ratio of variance between countries to the variance within countries, and these results suggest there are statistically significant differences in auction prices between countries. However, the practical significance of these differences might be limited given the low R-squared values observed in the regression analysis.\n",
    "\n",
    "In summary, the analysis indicates that while there are statistically significant differences in auction prices among countries, the overall impact of country on auction prices is relatively small, explaining only a small fraction of the variation in prices. Other variables and factors not included in this analysis are likely to have a more substantial impact on auction prices. The slight differences in F-statistics between data versions suggest that removing sparse countries or grouping them into an \"Other\" category can slightly alter the statistical significance of country differences, but the overall conclusion remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e87c3-bfc1-4c0b-abff-5cb84382a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Making 'countryCode' and 'osAndVersion' categorical\n",
    "data_grouped_other['countryCode'] = data_grouped_other['countryCode'].astype('category')\n",
    "data_grouped_other['osAndVersion'] = data_grouped_other['osAndVersion'].astype('category')\n",
    "\n",
    "# Encoding the categorical variables numerically\n",
    "data_grouped_other['countryCode_encoded'] = data_grouped_other['countryCode'].cat.codes\n",
    "data_grouped_other['osAndVersion_encoded'] = data_grouped_other['osAndVersion'].cat.codes\n",
    "\n",
    "# Prepare your features (X) and target (y) for regression\n",
    "# Let's use 'countryCode_encoded' and 'osAndVersion_encoded' as part of our features\n",
    "X = data_grouped_other[['countryCode_encoded', 'osAndVersion_encoded']].values\n",
    "y = data_grouped_other['sentPrice'].values\n",
    "\n",
    "# Since we can't use KNN due to library constraints, let's consider a simple linear regression as an alternative:\n",
    "# Adding a column of ones to include the intercept term in the model\n",
    "X_with_intercept = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Performing the linear regression using the Normal Equation\n",
    "beta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y\n",
    "\n",
    "# Predicting y values using the regression coefficients\n",
    "y_pred = X_with_intercept @ beta\n",
    "\n",
    "# Calculating residuals\n",
    "residuals = y - y_pred\n",
    "\n",
    "# Calculating Total Sum of Squares (TSS) and Residual Sum of Squares (RSS)\n",
    "TSS = np.sum((y - np.mean(y)) ** 2)\n",
    "RSS = np.sum(residuals ** 2)\n",
    "\n",
    "# Calculating R-squared\n",
    "R_squared = 1 - (RSS / TSS)\n",
    "\n",
    "print(f\"R_squared: {R_squared}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79a6ac-2f7a-457b-8e4f-d8e959797423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_regression(X_train, y_train, X_test, k):\n",
    "    \"\"\"\n",
    "    K-Nearest Neighbors regression.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Feature set for training data.\n",
    "    - y_train: Target values for training data.\n",
    "    - X_test: Feature set for data to predict.\n",
    "    - k: Number of nearest neighbors to consider.\n",
    "\n",
    "    Returns:\n",
    "    - Predictions for X_test.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for efficient computation\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    \n",
    "    # Placeholder for predictions\n",
    "    predictions = np.zeros(X_test.shape[0])\n",
    "\n",
    "    # Iterate over each item in X_test to make predictions\n",
    "    for i, test_point in enumerate(X_test):\n",
    "        # Calculate Euclidean distances from this test point to all training points\n",
    "        distances = np.sqrt(np.sum((X_train - test_point) ** 2, axis=1))\n",
    "        \n",
    "        # Get indices of k smallest distances\n",
    "        nearest_neighbor_ids = np.argsort(distances)[:k]\n",
    "        \n",
    "        # Average the target values of nearest neighbors\n",
    "        predictions[i] = np.mean(y_train[nearest_neighbor_ids])\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "# def knn_regression(X_train, y_train, X_test, k):\n",
    "#     \"\"\"\n",
    "#     K-Nearest Neighbors regression.\n",
    "\n",
    "#     Parameters:\n",
    "#     - X_train: Feature set for training data.\n",
    "#     - y_train: Target values for training data.\n",
    "#     - X_test: Feature set for data to predict.\n",
    "#     - k: Number of nearest neighbors to consider.\n",
    "\n",
    "#     Returns:\n",
    "#     - Predictions for X_test.\n",
    "#     \"\"\"\n",
    "#     # Calculate pairwise Euclidean distances between training and test points\n",
    "#     distances = np.sqrt(np.sum((X_train[:, np.newaxis] - X_test) ** 2, axis=2))\n",
    "    \n",
    "#     # Get indices of k smallest distances for each test point\n",
    "#     nearest_neighbor_ids = np.argsort(distances, axis=0)[:k]\n",
    "    \n",
    "#     # Average the target values of nearest neighbors for each test point\n",
    "#     predictions = np.mean(y_train[nearest_neighbor_ids], axis=0)\n",
    "    \n",
    "#     return predictions\n",
    " \n",
    "# Let's split data_grouped_other into a simple train-test split for demonstration\n",
    "np.random.seed(42)  # For reproducibility\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "split_idx = int(len(y) * 0.8)  # 80% train, 20% test split\n",
    "\n",
    "X_train = X_with_intercept[shuffle_indices][:split_idx]\n",
    "y_train = y[shuffle_indices][:split_idx]\n",
    "X_test = X_with_intercept[shuffle_indices][split_idx:]\n",
    "y_test = y[shuffle_indices][split_idx:]\n",
    "\n",
    "# Perform KNN regression\n",
    "# Function to calculate MSE\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Range of k values to try\n",
    "# k_values = range(1, 21)  # For example, trying k from 1 to 20\n",
    "k_values = range(4, 10) \n",
    "# Store MSE for each k\n",
    "mse_values = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(k)\n",
    "    y_pred = knn_regression(X_train, y_train, X_test, k)\n",
    "    mse = calculate_mse(y_test, y_pred)\n",
    "    mse_values.append(mse)\n",
    "\n",
    "# Find the optimal k (with the lowest MSE)\n",
    "optimal_k = k_values[np.argmin(mse_values)]\n",
    "optimal_mse = min(mse_values)\n",
    "\n",
    "print(f\"Optimal k: {optimal_k} with MSE: {optimal_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7f722-d70a-4148-b0de-7e94f68df984",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k_predictions = knn_regression(X_train, y_train, X_test, optimal_k)\n",
    "\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def calculate_mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def calculate_r_squared(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "# Calculate metrics for optimal k\n",
    "rmse = calculate_rmse(y_test, optimal_k_predictions)\n",
    "mae = calculate_mae(y_test, optimal_k_predictions)\n",
    "r_squared = calculate_r_squared(y_test, optimal_k_predictions)\n",
    "\n",
    "print(f\"RMSE for optimal k ({optimal_k}): {rmse}\")\n",
    "print(f\"MAE for optimal k ({optimal_k}): {mae}\")\n",
    "print(f\"R-squared for optimal k ({optimal_k}): {r_squared}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbcffbd-e259-4ddb-bccc-a41c4f92d2ef",
   "metadata": {},
   "source": [
    "The improvement in the R-squared value from 0.0139 to 0.0253 when using KNN regression with the optimal k suggests that incorporating the nearest neighbors' information does offer a better fit for predicting auction prices compared to a simple linear regression model. It indicates that the model is capturing a bit more of the variability in auction prices by considering the similarity between observations.\n",
    "\n",
    "The RMSE and MAE provide insights into the average error magnitude of your predictions:\n",
    "\n",
    "RMSE (7.37): This value suggests that, on average, the model's predictions are about 7.37 units away from the actual auction prices in the dataset. Since RMSE penalizes larger errors more heavily, this also implies that there are some predictions with substantial errors, but it gives a somewhat balanced view of the overall prediction error.\n",
    "\n",
    "MAE (1.81): The Mean Absolute Error provides a more direct interpretation of the average prediction error, indicating that, on average, the predictions are off by approximately 1.81 units from the actual values. It's less sensitive to outliers than RMSE, providing a straightforward measure of prediction accuracy without overly penalizing larger errors.\n",
    "\n",
    "R-squared (0.0253): The Coefficient of Determination, although still relatively low, has nearly doubled from the linear regression model. It suggests that the KNN model, with the specified number of neighbors, is somewhat more effective at capturing the variance in auction prices than a straightforward linear approach.\n",
    "\n",
    "These results underline the nuanced nature of predicting auction prices and highlight the potential benefits of exploring non-linear and more complex models like KNN regression. The modest improvement in R-squared, while still leaving a lot of variances unexplained, points towards the complex dynamics at play in auction price determination. It suggests that further model refinement, possibly incorporating additional features or exploring more advanced modeling techniques, could yield even better insights and predictive performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b075d9-ca4d-4ec6-b255-5405e25c9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming auctions_df has 'bundleId' and 'sentPrice', and similarity_df has bundleIds as both index and columns\n",
    "\n",
    "# Merging the similarity scores with the sentPrice on bundleId\n",
    "merged_df = pd.merge(similarity_df.reset_index(), auctions_df, on='bundleId', how='inner')\n",
    "\n",
    "# Before calculating the average similarity, make sure to exclude any non-numeric columns.\n",
    "numeric_columns = merged_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Now you can safely drop 'bundleId' if it's not numeric and calculate the mean\n",
    "merged_df['average_similarity'] = merged_df[numeric_columns].mean(axis=1)\n",
    "\n",
    "# Calculate the correlation between 'average_similarity' and 'sentPrice'\n",
    "correlation = merged_df[['average_similarity', 'sentPrice']].corr().iloc[0, 1]\n",
    "print(\"Correlation between description similarity and sentPrice:\", correlation)\n",
    "\n",
    "print(\"Correlation between average description similarity and sentPrice:\", correlation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd58cf-7555-41c5-bdc6-1219e2b29cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b61a69-83f0-4325-aeb7-9c0bfe70a79e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Descriptive statistics for numerical columns\n",
    "descriptive_stats = merged_df.describe()\n",
    "\n",
    "# Value counts for each column (typically for categorical data)\n",
    "value_counts_per_column = {col: merged_df[col].value_counts() for col in merged_df.columns if merged_df[col].dtype == 'object'}\n",
    "\n",
    "# Number of unique values for each column\n",
    "unique_values_per_column = merged_df.nunique()\n",
    "\n",
    "# Data types of each column\n",
    "column_data_types = merged_df.dtypes\n",
    "\n",
    "print(\"Descriptive Statistics:\\n\", descriptive_stats)\n",
    "print(\"\\nValue Counts per Column:\")\n",
    "for col, counts in value_counts_per_column.items():\n",
    "    print(f\"\\nValue counts for {col}:\")\n",
    "    print(counts)\n",
    "\n",
    "print(\"\\nUnique Values per Column:\\n\", unique_values_per_column)\n",
    "print(\"\\nColumn Data Types:\\n\", column_data_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc98ed-2f60-4be5-9636-dbd5d142997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "# 1. Remove 'deviceId' and 'bundleId' columns\n",
    "data = merged_df.drop(['deviceId', 'bundleId'], axis=1)\n",
    "\n",
    "# 2. Categorize 'countryCode' with threshold\n",
    "country_counts = data['countryCode'].value_counts()\n",
    "data['countryCode'] = data['countryCode'].apply(lambda x: x if country_counts[x] > 1 else 'Other')\n",
    "\n",
    "# 3. Categorize 'brandName' with threshold\n",
    "brand_counts = data['brandName'].value_counts()\n",
    "data['brandName'] = data['brandName'].apply(lambda x: x if brand_counts[x] > 1 else 'Other')\n",
    "\n",
    "# 4. Transform 'osAndVersion'\n",
    "data['OS_Type'] = data['osAndVersion'].apply(lambda x: 'iOS' if 'iOS' in x else ('Android' if 'Android' in x else 'Other'))\n",
    "version_counts = data['osAndVersion'].value_counts()\n",
    "data['OS_Version'] = data['osAndVersion'].apply(lambda x: x if version_counts[x] > 1 else f\"{data['OS_Type']} Other\")\n",
    "data = data.drop('osAndVersion', axis=1)\n",
    "\n",
    "# 5. Transform 'eventTimestamp'\n",
    "data['eventTimestamp'] = pd.to_datetime(data['eventTimestamp'], unit='ms', utc=True)\n",
    "data['Weekday'] = data['eventTimestamp'].dt.weekday\n",
    "data['Hour'] = data['eventTimestamp'].dt.hour\n",
    "data['Is_Weekend'] = data['Weekday'].apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')\n",
    "\n",
    "# Mapping numbers to actual day names\n",
    "data['Day_of_Week'] = data['eventTimestamp'].dt.day_name()\n",
    "\n",
    "# Dropping the 'eventTimestamp' as it's no longer needed\n",
    "data = data.drop('eventTimestamp', axis=1)\n",
    "\n",
    "# Ensuring the 'countryCode' and 'brandName' columns are of category type\n",
    "data['countryCode'] = data['countryCode'].astype('category')\n",
    "data['brandName'] = data['brandName'].astype('category')\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f197afb-13f9-48ef-b66c-608d2c03c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'countryCode' into numerical categories\n",
    "data['countryCode'] = pd.Categorical(data['countryCode'])\n",
    "data['countryCode'] = data['countryCode'].cat.codes\n",
    "\n",
    "# Converting 'brandName' into numerical categories\n",
    "data['brandName'] = pd.Categorical(data['brandName'])\n",
    "data['brandName'] = data['brandName'].cat.codes\n",
    "\n",
    "# Converting 'OS_Type' into numerical categories\n",
    "data['OS_Type'] = pd.Categorical(data['OS_Type'])\n",
    "data['OS_Type'] = data['OS_Type'].cat.codes\n",
    "\n",
    "# Converting 'OS_Version' into numerical categories\n",
    "data['OS_Version'] = pd.Categorical(data['OS_Version'])\n",
    "data['OS_Version'] = data['OS_Version'].cat.codes\n",
    "\n",
    "# For 'Day_of_Week' use a mapping from day names to numbers\n",
    "day_mapping = {day: i for i, day in enumerate(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])}\n",
    "data['Day_of_Week'] = data['Day_of_Week'].map(day_mapping)\n",
    "\n",
    "# For 'Is_Weekend' convert to binary numerical category\n",
    "weekend_mapping = {'Weekday': 0, 'Weekend': 1}\n",
    "data['Is_Weekend'] = data['Is_Weekend'].map(weekend_mapping)\n",
    "\n",
    "columns_to_scale = ['average_similarity', 'sentPrice','bidFloorPrice'] #Normalization is typically applied to continuous data, not categorical data, unless those categories have a meaningful order \n",
    "for column in columns_to_scale:\n",
    "    data[column] = (data[column] - data[column].min()) / (data[column].max() - data[column].min())\n",
    "\n",
    "data['unitDisplayType'] = data['unitDisplayType'].astype('category')\n",
    "data['unitDisplayType'] = data['unitDisplayType'].cat.codes\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485cdff5-ae55-43fb-8da9-b6ab410f8f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51c822-68ac-4e81-8e49-a11f33f02340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def knn_regression(X_train, y_train, X_test, k):\n",
    "    predictions = []\n",
    "    for test_point in X_test.to_numpy():\n",
    "        # Reshape test_point for subtraction\n",
    "        test_point = test_point.reshape(1, -1)\n",
    "\n",
    "        # Calculate distances\n",
    "        distances = np.linalg.norm(X_train.to_numpy() - test_point, axis=1)\n",
    "\n",
    "        # Find the indices of the k nearest neighbors\n",
    "        k_indices = np.argpartition(distances, k)[:k]\n",
    "\n",
    "        # Average the target values of the nearest neighbors\n",
    "        predictions.append(np.mean(y_train.iloc[k_indices]))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def manual_train_test_split(X, y, test_size=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    shuffled_indices = np.random.permutation(len(X))\n",
    "    test_set_size = int(len(X) * test_size)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return X.iloc[train_indices], X.iloc[test_indices], y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "def mean_squared_error_manual(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def r2_score_manual(y_true, y_pred):\n",
    "    total_variance = np.var(y_true)\n",
    "    explained_variance = np.var(y_true - y_pred)\n",
    "    return 1 - (explained_variance / total_variance)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into X and y\n",
    "X = data.drop('sentPrice', axis=1)  # Features\n",
    "y = data['sentPrice']  # Target variable\n",
    "X_train, X_test, y_train, y_test = manual_train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Convert the first row (predict_point) to a 2D array or ensure it's in the correct shape\n",
    "predict_point = X.iloc[0].values.reshape(1, -1)\n",
    "\n",
    "# Use the rest of the dataset for prediction\n",
    "features = X.iloc[1:]  # excluding the first point to avoid zero distance\n",
    "targets = y.iloc[1:]  # excluding the corresponding target\n",
    "\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "k_values = range(1, 21)  # Example: trying k from 1 to 20\n",
    "\n",
    "for k in k_values:\n",
    "    y_pred = knn_regression(X_train, y_train, X_test, k)\n",
    "    mse = mean_squared_error_manual(y_test, y_pred)\n",
    "    r2 = r2_score_manual(y_test, y_pred)\n",
    "    \n",
    "    mse_scores.append(mse)\n",
    "    r2_scores.append(r2)\n",
    "    print(f\"k={k}: MSE={mse}, R^2={r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713530d-613d-476f-bd24-614691d79787",
   "metadata": {},
   "source": [
    "To summarize the journey and analysis we've embarked on, focusing on enhancing our KNN regression model and exploring the relationship between app description similarity and sent price, we've made significant progress and uncovered interesting findings. Our methodology was constrained by the tools available—namely, NumPy and Pandas—limiting our ability to employ external libraries for machine learning, visualization, and advanced statistical analysis. Despite these constraints, our approach was methodical and insightful, leveraging the core capabilities of these libraries to their fullest.\n",
    "\n",
    "Summary of Changes and Findings:\n",
    "Introduction of Embeddings and Cosine Similarity: We started by introducing text embeddings and calculating cosine similarity to measure the similarity between app descriptions. This innovative approach allowed us to capture nuanced semantic relationships that traditional numerical and categorical data might overlook.\n",
    "\n",
    "Normalization and Preprocessing: Recognizing the importance of normalization in distance-based algorithms like KNN, we applied column-wise normalization to our features. This step ensured that no single feature would disproportionately influence the model due to scale differences.\n",
    "\n",
    "Optimal K Identification: Through a meticulous process of evaluating different values of k, we determined that k=6 provided the best balance, yielding the lowest mean squared error (MSE) of 54.3890041103897 for our initial set of features. This finding underscored the importance of tuning hyperparameters, even in relatively simple models like KNN.\n",
    "\n",
    "Feature Engineering and Selection: We undertook feature engineering to transform and categorize our data further. This included converting the osAndVersion column into more granular and informative features and categorizing countries and brands with low occurrence under a common \"other\" category to reduce sparsity and potential overfitting.\n",
    "\n",
    "Comparing Analysis and Identifying Future Directions:\n",
    "Comparing our initial analysis with the subsequent steps involving embeddings and similarity scores, it's clear that incorporating text data and semantic analysis can add valuable dimensions to our model. The initial MSE provided a benchmark, and while our exploration into embeddings and similarity scores introduced complexity, it also opened avenues for deeper understanding and more nuanced predictions.\n",
    "\n",
    "Future Directions:\n",
    "Feature Relevance Evaluation: Without access to plotting libraries or advanced statistical packages, we can still delve deeper into feature relevance through correlation analysis and manual inspection. Identifying and potentially removing irrelevant features could streamline the model and improve performance.\n",
    "\n",
    "Cross-validation: Implementing a manual cross-validation process could provide a more robust evaluation of our model's performance across different subsets of the data, reducing the variance of our MSE estimate and offering insights into the model's stability.\n",
    "\n",
    "Combining Models: While outside the scope of our current toolset, considering ensemble methods or combining predictions from models trained on different subsets of features (textual similarity versus traditional numerical/categorical data) could yield improvements in accuracy.\n",
    "\n",
    "Textual Data Exploration: Further refinement of the embedding process or exploring different techniques for generating text embeddings could enhance the model's ability to capture the nuances of app descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e7c3a-09b2-46f9-ac46-50297c958a3a",
   "metadata": {},
   "source": [
    "considering the constraints and the progress we've made, focusing on feature importance and selection through methods like correlation and mutual information is a prudent step forward. Let's delve into how these approaches could be incorporated into our analysis and future steps:\n",
    "\n",
    "Feature Importance via Correlation:\n",
    "Correlation Analysis: Using Pearson correlation coefficients, we can identify how each feature is linearly related to the target variable (sentPrice). Features with very low correlation coefficients might be considered less important and potentially excluded from the model to improve performance and reduce complexity.\n",
    "Heatmaps for Visualization: Although our current constraints limit the use of external libraries for visualization, under different circumstances, heatmaps would be an excellent tool for visually identifying relationships between all variables at once.\n",
    "Feature Selection via Mutual Information:\n",
    "Mutual Information (MI): This technique measures the amount of information one can obtain from one variable through another. Unlike correlation, MI can capture non-linear relationships, making it a powerful tool for feature selection, especially for complex datasets where linear relationships might not encapsulate the full picture.\n",
    "Implementation: We can manually implement a simplified version of mutual information calculation using NumPy and Pandas. This would involve discretizing continuous features and employing entropy-based measures to evaluate the dependency between each feature and the target variable.\n",
    "Future Steps with Feature Importance and Selection:\n",
    "Iterative Feature Elimination: Based on the insights from correlation and mutual information analyses, we can iteratively remove the least important features and evaluate how the model's performance changes. This approach helps in identifying a minimal set of features that still yields an optimal or near-optimal prediction accuracy.\n",
    "Exploring Non-linear Relationships: Given the limitations of linear correlation analysis, applying mutual information could unveil hidden patterns and dependencies in the data, leading to more informed feature selection and potentially better model performance.\n",
    "Practical Considerations:\n",
    "Manual Calculations and Simplifications: Given the tool constraints, we may need to simplify some calculations or manually implement certain metrics. While this approach is more labor-intensive, it allows for deeper understanding and control over the analysis process.\n",
    "Focus on Scalable Techniques: As we explore these methods, it's essential to consider the scalability of manual implementations, especially for large datasets. Efficient data manipulation and calculation strategies will be crucial.\n",
    "Incorporating these advanced feature selection techniques could significantly enhance our model's predictive power and efficiency. By systematically evaluating and selecting features based on their relevance to the target variable, we can streamline the model, reduce overfitting, and potentially uncover more nuanced insights into the factors driving app sent prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a89745-0f16-404f-b0c3-06119948133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let start with correlation:\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Extract correlations with 'sentPrice', excluding the correlation of 'sentPrice' with itself\n",
    "sentPrice_correlations = correlation_matrix['sentPrice'].drop(['sentPrice','id')\n",
    "\n",
    "# Sort the correlations by their absolute values in descending order\n",
    "sorted_correlations = sentPrice_correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "print(sorted_correlations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a2d9b-2962-4500-b769-25e63677b012",
   "metadata": {},
   "source": [
    "From this correlation analysis with sentPrice, we can glean several insights about the relationship between your features and the target variable. Here's a summary of key takeaways and what they imply for your predictive modeling efforts:\r\n",
    "\r\n",
    "unitDisplayType (0.188479): Shows the highest correlation with sentPrice among all features, suggesting that the type of ad unit significantly influences the price. This could be due to differences in ad effectiveness, visibility, or user engagement across unit types.\r\n",
    "\r\n",
    "bidFloorPrice (0.114992): This is somewhat expected, as the minimum bid price set for an auction might directly influence the final sent price, indicating a positive relationship between the floor price and the final transaction price.\r\n",
    "\r\n",
    "Geographical and Brand Factors: Both countryCode (0.100172) and brandName (0.086067) have notable correlations with sentPrice, indicating that the country in which an ad is displayed and the brand of the device can affect ad pricing. This might reflect market dynamics, purchasing power, or brand preferences.\r\n",
    "\r\n",
    "OS_Type and OS_Version: These factors show a lower degree of correlation but are still notable. They suggest that the operating system type and version might have a small impact on sentPrice, potentially due to differences in user demographics or behavior across different OS platforms.\r\n",
    "\r\n",
    "Embedding Vectors: The embedding vector columns (like 1436213906, se.ace.fishinc, etc.) show varying degrees of correlation, with some being more strongly related to sentPrice than others. This variability suggests that the content or context represented by these embeddings (possibly related to app descriptions or user interests) can have a differential impact on ad pricing.\r\n",
    "\r\n",
    "Temporal Features (Hour, Weekday, Is_Weekend, Day_of_Week): Hour shows a small correlation, suggesting that the time of day might slightly influence ad prices, possibly due to varying user engagement levels throughout the day. However, Weekday, Is_Weekend, and Day_of_Week show very low to no correlation, indicating that the day of the week may not significantly impact sentPrice.\r\n",
    "\r\n",
    "Low Correlation Features: Features like average_similarity have very low correlation values, suggesting they have minimal linear relationship with sentPrice. This doesn't mean they are irrelevant, but they might not contribute significantly to a model that relies on linear relationships.\r\n",
    "\r\n",
    "NaN Values: Is_Weekend and Day_of_Week showing NaN might indicate that these variables have no variation or an issue in computation. If they're constant or nearly constant, they wouldn't contribute to the model's predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6922399-5f5d-48f4-aad8-68fa4961157e",
   "metadata": {},
   "source": [
    "let try now implementing a simplified version of mutual information calculation using NumPy and Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac87f940-109a-4fcc-b8ba-03a0f42d6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    probabilities = y.value_counts(normalize=True)\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "def calculate_conditional_entropy(x, y):\n",
    "    # Group the data by feature and calculate the entropy of the target for each group\n",
    "    conditional_entropy = 0.0\n",
    "    for value in x.unique():\n",
    "        y_subset = y[x == value]\n",
    "        conditional_entropy += (len(y_subset) / len(y)) * calculate_entropy(y_subset)\n",
    "    return conditional_entropy\n",
    "\n",
    "def calculate_mutual_information(x, y):\n",
    "    return calculate_entropy(y) - calculate_conditional_entropy(x, y)\n",
    "\n",
    "mi_scores = {}\n",
    "for column in data.columns.drop('sentPrice'):\n",
    "    if data[column].dtype == 'float64':  # Assuming continuous variables are float64\n",
    "        data[column] = pd.cut(data[column], bins=10, labels=False)\n",
    "    mi_scores[column] = calculate_mutual_information(data[column], data['sentPrice'])\n",
    "\n",
    "# Sort features by MI score\n",
    "sorted_mi_scores = sorted(mi_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feature, score in sorted_mi_scores:\n",
    "    print(f\"{feature}: {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc83cb4-c80d-4ed6-89fd-ee3370275bca",
   "metadata": {},
   "source": [
    "Mutual information is particularly useful because, unlike correlation, it can capture both linear and non-linear relationships between variables. Here's what we can infer from your results (excluding the id part):\n",
    "\n",
    "High Mutual Information Scores: Features like countryCode, com.kamilbilge.ropesavior3d, and time-related features (Hour, Weekday, Day_of_Week) have higher mutual information scores. This suggests these features share a significant amount of information with the target variable and are potentially very relevant for predicting sentPrice. For example, countryCode having a high score implies the geographical location of the auction significantly impacts the sent price.\n",
    "\n",
    "Application and Game Identifiers: Specific application or game identifiers (e.g., com.kamilbilge.ropesavior3d, 1579489488, com.AppIdeas.LevelUpRunner) having high scores indicate that certain applications or games are more predictive of the target variable. This could be due to specific characteristics or popularity of these apps/games affecting the price.\n",
    "\n",
    "Brand, OS, and Device Attributes: Features like brandName and OS_Version having relatively high mutual information scores suggest that the device brand and the operating system version also play a role in determining sentPrice. This could be tied to user demographics, preferences, or the technical capabilities of devices that influence advertising costs.\n",
    "\n",
    "Temporal Features: Hour, Weekday, and Day_of_Week showing importance highlights the impact of the timing of auctions on the sent price. This aligns with typical bidding behavior where certain times of the day or week may see higher engagement and therefore higher prices.\n",
    "\n",
    "Lower Scores for Some Features: Features like OS_Type and bidFloorPrice showing lower mutual information scores might indicate they are less directly related to sentPrice compared to other features. However, they still provide some information and shouldn't be dismissed without further analysis.\n",
    "\n",
    "Average Similarity: The average_similarity having a certain level of mutual information suggests that the similarity of app descriptions or other textual content might have some impact on the price, though it's not among the top predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4f08f6-4416-46c9-a7ab-3531dbfe94f2",
   "metadata": {},
   "source": [
    "My last idea is to use  polynomial and trigonometric feature transformations that can in turn  help capture more complex relationships between features and the target variable, \n",
    "sentPrice, potentially improving the performance of KNN regression and informing feature selection through methods like mutual information.\r\n",
    "\r\n",
    "KNN Regression: KNN relies on the proximity of samples in the feature space to make predictions. Non-linear transformations like polynomial and trigonometric functions can change the geometry of the feature space, making patterns more discernible for KNN. This could lead to more accurate predictions, especially if the underlying relationship betw eendata a\r\n",
    "�\r\n",
    "�\r\n",
    "sentPrice is non-linear.\r\n",
    "\r\n",
    "Feature Selection through Mutual Information: Mutual information measures the dependency between variables, capturing both linear and non-linear relationships. By transforming features, you might reveal or amplify underlying patterns that mutual information can detect, leading to a more informed feature selection process. Features that seemed less informative in their original form might show higher mutual information with the target variable after transformation, indicating their hidden relevance.\r\n",
    "\r\n",
    "Feature Elimination: By examining the mutual information or performance of KNN regression before and after feature transformations, you can make more informed decisions about which features to keep or dis �\r\n",
    "�\r\n",
    "�\r\n",
    "�\r\n",
    "�\r\n",
    "�\r\n",
    "�\r\n",
    "sentPrice after transformations can be considered for elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f246bd62-4da5-4e2f-b7d0-63122d27e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def apply_transformations(df, trig_degree=2, poly_degree=3):\n",
    "    original_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    trig_functions = [np.sin, np.cos]\n",
    "\n",
    "    # Trigonometric Transformations\n",
    "    for degree in range(1, trig_degree + 1):\n",
    "        for col in original_columns:\n",
    "            for func in trig_functions:\n",
    "                df[f'{func.__name__}{degree}({col})'] = func(degree * df[col])\n",
    "\n",
    "    # Polynomial/Binomial Expansion-like Feature Combinations\n",
    "    for degree in range(2, poly_degree + 1):\n",
    "        # for comb in combinations_with_replacement(df.select_dtypes(include=[np.number]).columns, degree):\n",
    "        for comb in combinations_with_replacement(original_columns, degree):\n",
    "            new_col_name = '*'.join(comb)\n",
    "            df[new_col_name] = df[list(comb)].prod(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the transformations with specified degrees\n",
    "features_df = data.drop(['sentPrice','id'], axis=1)  # Exclude the target variable\n",
    "\n",
    "# Apply transformations to the features DataFrame\n",
    "transformed_features = apply_transformations(features_df)\n",
    "\n",
    "transformed_features.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cef280-32d3-414c-810f-d9793d0e9da3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a0646-ae31-4e3c-ba94-6f735be03f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_scores = {}\n",
    "for column in transformed_features.columns:\n",
    "    if transformed_features[column].dtype == 'float64':  # Assuming continuous variables are float64\n",
    "        transformed_features[column] = pd.cut(transformed_features[column], bins=10, labels=False)\n",
    "    mi_scores[column] = calculate_mutual_information(transformed_features[column], data['sentPrice'])\n",
    "\n",
    "# Sort features by MI score\n",
    "sorted_mi_scores = sorted(mi_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feature, score in sorted_mi_scores:\n",
    "    print(f\"{feature}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489ae8a-562a-4c76-89c2-1748ea2a2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_bin*Hour*Hour: 1.0378333481904916\n",
    "similarity_bin*similarity_bin*Hour: 1.0325525965840843\n",
    "similarity_bin*Weekday*Hour: 0.9154877525415914\n",
    "similarity_bin*Hour: 0.8476902936790456\n",
    "Weekday*Hour*Hour: 0.8180888271164592\n",
    "Weekday*Weekday*Hour: 0.7752092759834417\n",
    "countryCode: 0.6938422926255212\n",
    "similarity_bin*similarity_bin*Weekday: 0.6675920436528662\n",
    "Weekday*Hour: 0.6410068584806856\n",
    "similarity_bin*Weekday*Weekday: 0.6269447216673258\n",
    "average_similarity*average_similarity*similarity_bin: 0.5710774608238571\n",
    "average_similarity*average_similarity_countryCode*similarity_bin: 0.5408681034773108\n",
    "average_similarity_countryCode*average_similarity_countryCode*similarity_bin: 0.5216506424394636\n",
    "average_similarity_countryCode*average_similarity_osAndVersion*similarity_bin: 0.5141077561354912\n",
    "average_similarity_osAndVersion*average_similarity_osAndVersion*similarity_bin: 0.5069109055455385\n",
    "average_similarity_osAndVersion*similarity_bin: 0.5044684474638119\n",
    "average_similarity_countryCode*similarity_bin: 0.5041126391077411\n",
    "average_similarity*average_similarity_osAndVersion*similarity_bin: 0.5036094608715143\n",
    "similarity_bin: 0.5025044593194421\n",
    "average_similarity*similarity_bin: 0.5025044593194421\n",
    "similarity_bin*similarity_bin: 0.5025044593194421\n",
    "similarity_bin*similarity_bin*similarity_bin: 0.5025044593194421\n",
    "sin2(average_similarity): 0.499921646906313\n",
    "sin2(bidFloorPrice): 0.49237474449954544\n",
    "similarity_bin*Weekday: 0.46623052549888566\n",
    "cos1(similarity_bin): 0.4626430749839159\n",
    "average_similarity*average_similarity*average_similarity_osAndVersion: 0.45851103260203363\n",
    "average_similarity_osAndVersion*similarity_bin*similarity_bin: 0.44809050413166673\n",
    "average_similarity_countryCode*similarity_bin*similarity_bin: 0.44764521043854266\n",
    "average_similarity*similarity_bin*similarity_bin: 0.4466140485907433\n",
    "average_similarity*average_similarity*average_similarity_countryCode: 0.41716399774443236\n",
    "average_similarity*average_similarity: 0.4170837345544509\n",
    "average_similarity*average_similarity*average_similarity: 0.4170837345544509\n",
    "cos1(average_similarity): 0.40197707723416\n",
    "cos2(average_similarity): 0.40197707723416\n",
    "average_similarity: 0.39900204158753905\n",
    "sin1(average_similarity): 0.39900204158753905\n",
    "Hour: 0.38837384371850714\n",
    "Hour*Hour: 0.38837384371850714\n",
    "Hour*Hour*Hour: 0.38837384371850714\n",
    "average_similarity*average_similarity_osAndVersion: 0.3545405663727381\n",
    "sin2(similarity_bin): 0.3498810880669936\n",
    "brandName: 0.3493953262154017\n",
    "OS_Version: 0.34464456239160857\n",
    "average_similarity*average_similarity_countryCode: 0.3354092786226426\n",
    "sin1(similarity_bin): 0.3278253157300153\n",
    "cos2(similarity_bin): 0.31719786282526385\n",
    "average_similarity*average_similarity_countryCode*average_similarity_osAndVersion: 0.3046814481102924\n",
    "unitDisplayType: 0.2928241709638675\n",
    "average_similarity*average_similarity_osAndVersion*average_similarity_osAndVersion: 0.28680101387959933\n",
    "average_similarity*average_similarity_countryCode*Weekday: 0.27010508013568657\n",
    "average_similarity_osAndVersion*similarity_bin*Hour: 0.2698798136996876\n",
    "average_similarity_countryCode*similarity_bin*Hour: 0.26862176823579276\n",
    "average_similarity*similarity_bin*Hour: 0.26643763718068136\n",
    "average_similarity*average_similarity_countryCode*average_similarity_countryCode: 0.2589782353169232\n",
    "average_similarity*average_similarity*Weekday: 0.2450634843753301\n",
    "average_similarity_countryCode*similarity_bin*Weekday: 0.24495010662533012\n",
    "average_similarity*Weekday: 0.24398177276087907\n",
    "average_similarity*similarity_bin*Weekday: 0.23815678414916164\n",
    "average_similarity_osAndVersion*similarity_bin*Weekday: 0.23777236302990712\n",
    "average_similarity*average_similarity*Hour: 0.22553330728193277\n",
    "average_similarity*Hour: 0.22436931604582533\n",
    "average_similarity*average_similarity_osAndVersion*Hour: 0.22422792799215774\n",
    "average_similarity*average_similarity_countryCode*Hour: 0.22415098956899016\n",
    "average_similarity_osAndVersion*average_similarity_osAndVersion*Hour: 0.22139844062585734\n",
    "average_similarity_countryCode*average_similarity_countryCode*Hour: 0.22103603480817569\n",
    "average_similarity_countryCode*Hour: 0.2203687661405782\n",
    "average_similarity_countryCode*average_similarity_osAndVersion*Hour: 0.22017335273761418\n",
    "average_similarity*average_similarity_osAndVersion*Weekday: 0.22003662587416262\n",
    "average_similarity_osAndVersion*Hour: 0.21936276570602065\n",
    "average_similarity*Hour*Hour: 0.21294308619314517\n",
    "average_similarity_countryCode*Hour*Hour: 0.2106716201675427\n",
    "average_similarity_osAndVersion*Hour*Hour: 0.20926591627839564\n",
    "sin2(Hour): 0.20879483057998893\n",
    "cos1(Hour): 0.2014318373382311\n",
    "average_similarity*Weekday*Weekday: 0.19815468292854632\n",
    "average_similarity_countryCode*average_similarity_countryCode*Weekday: 0.1927068829341385\n",
    "average_similarity_countryCode*average_similarity_osAndVersion*Weekday: 0.18958297783343614\n",
    "sin1(Hour): 0.1887628568337405\n",
    "average_similarity_osAndVersion*Weekday*Hour: 0.18151567356999188\n",
    "average_similarity_countryCode*Weekday: 0.18087168914877338\n",
    "average_similarity_countryCode*Weekday*Hour: 0.1799559255818668\n",
    "average_similarity_osAndVersion*average_similarity_osAndVersion*Weekday: 0.17720694966107242\n",
    "average_similarity*Weekday*Hour: 0.1764837704768727\n",
    "average_similarity_osAndVersion*Weekday: 0.1748686872012275\n",
    "cos2(Hour): 0.17448376835412294\n",
    "Weekday: 0.17233976268350304\n",
    "Day_of_Week: 0.17233976268350304\n",
    "Weekday*Weekday: 0.17233976268350304\n",
    "Weekday*Weekday*Weekday: 0.17233976268350304\n",
    "cos1(average_similarity_countryCode): 0.15305937606430575\n",
    "cos1(Weekday): 0.1514373029344096\n",
    "average_similarity_countryCode*average_similarity_countryCode: 0.1509785998978952\n",
    "average_similarity_countryCode*average_similarity_osAndVersion: 0.15086258751587778\n",
    "average_similarity_osAndVersion*Weekday*Weekday: 0.15011564396370503\n",
    "average_similarity_countryCode*Weekday*Weekday: 0.14860140505488406\n",
    "average_similarity_countryCode*average_similarity_countryCode*average_similarity_countryCode: 0.148166231758327\n",
    "sin1(average_similarity_countryCode): 0.14575222561069445\n",
    "average_similarity_countryCode: 0.14561665928690193\n",
    "cos2(average_similarity_countryCode): 0.14547772344680165\n",
    "average_similarity_countryCode*average_similarity_countryCode*average_similarity_osAndVersion: 0.1304803565349486\n",
    "sin2(Weekday): 0.12938625187738317\n",
    "cos2(Weekday): 0.12774275193075546\n",
    "sin2(average_similarity_countryCode): 0.12385902378829439\n",
    "sin1(Weekday): 0.12239570861789861\n",
    "average_similarity_countryCode*average_similarity_osAndVersion*average_similarity_osAndVersion: 0.10065591402386254\n",
    "sin2(average_similarity_osAndVersion): 0.08804006879264659\n",
    "average_similarity_osAndVersion: 0.08276992578419673\n",
    "cos1(average_similarity_osAndVersion): 0.08228346841637446\n",
    "average_similarity_osAndVersion*average_similarity_osAndVersion: 0.08223105701849942\n",
    "cos2(average_similarity_osAndVersion): 0.08213984459567047\n",
    "average_similarity_osAndVersion*average_similarity_osAndVersion*average_similarity_osAndVersion: 0.08184961650700728\n",
    "sin1(average_similarity_osAndVersion): 0.0790543336395979\n",
    "OS_Type: 0.0725347994395733\n",
    "bidFloorPrice*average_similarity_osAndVersion*Hour: 0.05370942905024112\n",
    "bidFloorPrice*Hour: 0.05348964621640917\n",
    "bidFloorPrice*bidFloorPrice*Hour: 0.05348964621640917\n",
    "bidFloorPrice*average_similarity*Hour: 0.05348964621640917\n",
    "bidFloorPrice*average_similarity_countryCode*Hour: 0.05348964621640917\n",
    "bidFloorPrice*similarity_bin*Hour: 0.05348964621640917\n",
    "bidFloorPrice*average_similarity_osAndVersion*average_similarity_osAndVersion: 0.050556214251455955\n",
    "bidFloorPrice: 0.05037345065875787\n",
    "sin1(bidFloorPrice): 0.05037345065875787\n",
    "cos1(bidFloorPrice): 0.05037345065875787\n",
    "cos2(bidFloorPrice): 0.05037345065875787\n",
    "bidFloorPrice*bidFloorPrice: 0.05037345065875787\n",
    "bidFloorPrice*average_similarity: 0.05037345065875787\n",
    "bidFloorPrice*average_similarity_countryCode: 0.05037345065875787\n",
    "bidFloorPrice*average_similarity_osAndVersion: 0.05037345065875787\n",
    "bidFloorPrice*similarity_bin: 0.05037345065875787\n",
    "bidFloorPrice*bidFloorPrice*bidFloorPrice: 0.05037345065875787\n",
    "bidFloorPrice*bidFloorPrice*average_similarity: 0.05037345065875787\n",
    "bidFloorPrice*bidFloorPrice*average_similarity_countryCode: 0.05037345065875787\n",
    "bidFloorPrice*bidFloorPrice*average_similarity_osAndVersion: 0.05037345065875787\n",
    "bidFloorPrice*bidFloorPrice*similarity_bin: 0.05037345065875787\n",
    "bidFloorPrice*average_similarity*average_similarity: 0.05037345065875787\n",
    "bidFloorPrice*average_similarity*average_similarity_countryCode: 0.05037345065875787\n",
    "bidFloorPrice*average_similarity*average_similarity_osAndVersion: 0.05037345065875787\n",
    "bidFloorPrice*average_similarity*similarity_bin: 0.05037345065875787\n",
    "bidFloorPrice*average_similarity_countryCode*average_similarity_countryCode: 0.05037345065875787\n",
    "bidFloorPrice*average_similarity_countryCode*average_similarity_osAndVersion: 0.05037345065875787\n",
    "bidFloorPrice*average_similarity_countryCode*similarity_bin: 0.05037345065875787\n",
    "bidFloorPrice*average_similarity_osAndVersion*similarity_bin: 0.05037345065875787\n",
    "bidFloorPrice*similarity_bin*similarity_bin: 0.05037345065875787\n",
    "bidFloorPrice*Weekday: 0.049388659039258265\n",
    "bidFloorPrice*bidFloorPrice*Weekday: 0.049388659039258265\n",
    "bidFloorPrice*average_similarity*Weekday: 0.049388659039258265\n",
    "bidFloorPrice*average_similarity_countryCode*Weekday: 0.049388659039258265\n",
    "bidFloorPrice*average_similarity_osAndVersion*Weekday: 0.049388659039258265\n",
    "bidFloorPrice*similarity_bin*Weekday: 0.049388659039258265\n",
    "bidFloorPrice*Hour*Hour: 0.0433520189741774\n",
    "bidFloorPrice*Weekday*Weekday: 0.040159553035404194\n",
    "bidFloorPrice*Weekday*Hour: 0.038140417206157906\n",
    "Is_Weekend: 0.037187577852753506"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38cd57-ee66-4820-be8d-a356fb2a2fe8",
   "metadata": {},
   "source": [
    "Embarking on this journey, we hypothesized that apps with similar descriptions would exhibit comparable asking prices in auctions, as denoted by the sentPrice column. To explore this, we employed cosine similarity to assess the similarity between app descriptions based on their embeddings and applied various statistical tools to substantiate or refute our hypothesis.\n",
    "\n",
    "Our initial steps involved calculating mutual information between features and sentPrice using the untransformed dataset, aiming to uncover direct contributions of each feature towards predicting auction prices. Here, we anticipated that embeddings reflecting similar descriptions would show a correlation with akin sentPrice values.\n",
    "\n",
    "Progressing further, we transformed the dataset, introducing new features through trigonometric and polynomial expansions, targeting to unveil more intricate relationships between the features and the target variable that linear assessments might overlook. This maneuver was predicated on the belief that if apps with analogous descriptions indeed share similar auction prices, these augmented features would elucidate this pattern more conspicuously.\n",
    "\n",
    "Post-transformation analysis via mutual information unveiled noteworthy insights. Particularly, transformed features frequently exhibited a heightened mutual dependency with sentPrice compared to the original set. This revelation hints at the possibility that the liaison between app description similarities (as encapsulated by the embeddings) and auction prices extends beyond simple linear connections, delving into more complex, potentially non-linear realms. The augmented mutual information scores for specific feature combinations, particularly those entailing transformed attributes, accentuated this complexity, suggesting that a blend of multiple features might yield a more precise prediction of auction prices.\n",
    "\n",
    "These outcomes intimate that the correlation between app description similarity and auction price transcends straightforward linear associations, involving intricate interactions among various features. Notably, transformed features like similarity_bin*Hour*Hour with a mutual information score of 1.0378333481904916 and similarity_bin*similarity_bin*Hour at 1.0325525965840843, stand out as significant. Their high mutual information values illuminate the intricate dynamics at play in predicting sentPrice, pointing towards the nuanced and multifaceted nature of this relationship.\n",
    "\n",
    "Reflecting on these insights compels us to reassess our initial hypothesis under a new light. The data suggests a nuanced relationship between app description similarity and auction prices, one that is captured through a complex interplay of both linear and non-linear feature interactions. Thus, our analysis, underpinned by cosine similarity and enriched through the statistical examination of transformed features, lends credence to our hypothesis, albeit revealing the intricate layers that govern the relationship between app description similarity and auction prices.\n",
    "\n",
    "As we forge ahead, our strategy will involve a meticulous refinement of the feature selection process, informed by the revelations from the mutual information analysis post-transformation. Concentrating on the most informative features—whether original or transformed—promises a path towards devising a more precise model capable of predicting auction prices from app description similarities effectively. This refined approach not only reiterates the potential validity of our hypothesis but also embarks us on an advanced exploration into the myriad factors influencing sentPrice in app auctions, thereby embracing the complexity and richness of the data at our disposal.\n",
    "\n",
    "\n",
    "its recomended no to run the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c263504-d376-4fa5-b7e9-ced32168160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "X = transformed_features  # Features\n",
    "y = data['sentPrice']  # Target variable\n",
    "X_train, X_test, y_train, y_test = manual_train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Convert the first row (predict_point) to a 2D array or ensure it's in the correct shape\n",
    "predict_point = X.iloc[0].values.reshape(1, -1)\n",
    "\n",
    "# Use the rest of the dataset for prediction\n",
    "features = X.iloc[1:]  # excluding the first point to avoid zero distance\n",
    "targets = y.iloc[1:]  # excluding the corresponding target\n",
    "\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "k_values = range(16, 21)  # Example: trying k from 16 to 20 \n",
    "\n",
    "for k in k_values:\n",
    "    y_pred = knn_regression(X_train, y_train, X_test, k)\n",
    "    mse = mean_squared_error_manual(y_test, y_pred)\n",
    "    r2 = r2_score_manual(y_test, y_pred)\n",
    "    \n",
    "    mse_scores.append(mse)\n",
    "    r2_scores.append(r2)\n",
    "    print(f\"k={k}: MSE={mse}, R^2={r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1df21-5eeb-42b7-9a1e-e92588c486c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=16: MSE=0.0002507651835986358, R^2=-0.03707803817177879\n",
    "k=17: MSE=0.0002501477558128505, R^2=-0.034522545479094724\n",
    "k=18: MSE=0.00024883147666771455, R^2=-0.029076953354937096\n",
    "k=19: MSE=0.00024833913993367525, R^2=-0.027034616955534885\n",
    "k=20: MSE=0.0002478244182300384, R^2=-0.02490592114466028"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a685a-33b9-4274-9ed8-84528acdd664",
   "metadata": {},
   "source": [
    "Following the transformation and introduction of new columns into our dataset, we embarked on a re-evaluation of the KNN regression model's performance. Notably, the results post-transformation demonstrated an improvement in the Mean Squared Error (MSE) across different values of �\r\n",
    "k, with a minimum MSE observed ak=\r\n",
    "2\r\n",
    "R \r\n",
    " of R^22\r\n",
    "  values provides insightful revelations into our modeling approach and hypothesis.\r\n",
    "\r\n",
    "We hypothesized that apps with similar descriptions would likely have similar asking prices in the auctions. This hypothesis was predicated on the assumption that cosine similarity, applied to the embeddings of app descriptions, could serve as a predictive indicator of auction prices. The initial analysis aimed to validate this hypothesis by leveraging statistical measures to explore the relationship between app description similarities and auction prices.\r\n",
    "\r\n",
    "The transformation process introduced new features through trigonometric and polynomial transformations, aiming to uncover more complex relationships within the data. This step was based on the assumption that the intrinsic relationship between app descriptions and auction prices might not be linear but could involve more intricate, nonlinear dynamics that our initial model did not fully capture.\r\n",
    "\r\n",
    "The observed improvement in MSE post-transformation suggests that the inclusion of these new features indeed facilitated a more nuanced understanding of the underlying patterns in the data. Specifically, the reduction in MSE indicates that the transformed dataset, with its augmented features, aligns more closely with the actual auction prices, thereby enhancing the model's predictive accuracy.\r\n",
    "\r\n",
    "R^2ment in \r\n",
    "�\r\n",
    "2\r\n",
    "R \r\n",
    "2\r\n",
    "  values, while indicative of a model that does not yet optimally predict auction prices, signals an interesting aspect of our hypothesis testing. The rR^2 eductioe change in \r\n",
    "�\r\n",
    "2\r\n",
    "R \r\n",
    "2\r\n",
    "  values underscore the complexity of the relationship between app descriptions and auction prices. It suggests that while we are moving closer to capturing the essence of this relationship, there remains a nuanced interplay of factors that needs to be unraveled.\r\n",
    "\r\n",
    "In interpreting these results, we acknowledge the pivotal role played by the transformed features in reducing the predictive error of our model. This reinforces our belief in the potential for complex feature interactions to more accurately reflect the dynamics between app descriptions and auction prices. Moving forward, we are encouraged to further refine our feature selection and transformation techniques to enhance the model's explana R^2td achieve positive \r\n",
    "�\r\n",
    "2\r\n",
    "R \r\n",
    "2\r\n",
    "  values, providing a more robust validation of our initial hypothesis.\r\n",
    "\r\n",
    "In summary, the observed improvement in MSE post-feature transformation and creation offers a promising direction for our analysis. It emphasizes the importance of embracing the data's complexity through thoughtful feature engineering, guiding us toward a deeper understanding of the intricate relationship between app description similarities and auction prices.SE and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d8108-94ad-4d6a-9a63-ff5c0a2f275e",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, the analysis focused on investigating the hypothesis that apps with similar descriptions would have similar asking prices in auctions, as represented by the 'sentPrice' column. The key steps and findings are as follows:\n",
    "Embeddings and Cosine Similarity: The analysis started by introducing text embeddings and calculating cosine similarity to measure the similarity between app descriptions. This approach allowed the capture of nuanced semantic relationships that traditional numerical and categorical data might have overlooked.\n",
    "Normalization and Preprocessing: The data was normalized to ensure that no single feature would disproportionately influence the model due to scale differences.\n",
    "KNN Regression Exploration: Despite initial hesitation about the computational complexity of KNN regression for the large dataset, the approach was ultimately implemented. The analysis evaluated different values of k and found that k=6 provided the best balance, yielding the lowest mean squared error (MSE).\n",
    "Feature Engineering and Selection: The notebook undertook feature engineering, transforming and categorizing the data further, including converting the 'osAndVersion' column into more granular features and handling low-occurrence countries and brands.\n",
    "Comparing Analyses and Identifying Future Directions: Comparing the initial analysis with the subsequent steps involving embeddings and similarity scores, it became evident that incorporating text data and semantic analysis can add valuable dimensions to the model. However, the overall low explanatory power of the models suggested the presence of other, unaccounted-for factors or more complex relationships influencing auction prices.\n",
    "Leveraging KNN Regression: The implementation of KNN regression, despite the initial concerns, led to an improvement in the R-squared value from 0.0139 to 0.0253. This suggested that the KNN model was better able to capture the variability in auction prices compared to the simpler linear regression approach.\n",
    "Future Directions: The analysis pointed towards several future directions, including feature relevance evaluation, cross-validation, combining models, and further exploration of textual data processing techniques to uncover the nuanced relationships between app description similarities and auction prices.\n",
    "The key takeaway is that the relationship between app description similarity and auction prices is more complex than a simple linear association. The incorporation of transformed features, such as trigonometric and polynomial expansions, revealed heightened mutual information scores, suggesting the presence of intricate, non-linear dynamics governing this relationship.\n",
    "While the initial hypothesis was partially supported, the limited model effectiveness signaled the need for a broader investigation. The pursuit of understanding the full spectrum of influences on auction prices remains open, suggesting avenues for future research to refine predictive models, incorporate a wider array of variables, and ultimately, unearth more comprehensive insights into the factors driving auction prices in the mobile app ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1b870-5fdb-4058-89e6-18ae0ebca92d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9675048c-31f6-451c-82e0-69f8068c1993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
